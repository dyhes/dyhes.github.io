<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hadoop on 飞鸿踏雪泥</title><link>https://dyhes.github.io/tags/hadoop/</link><description>Recent content in Hadoop on 飞鸿踏雪泥</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 26 Feb 2025 16:37:27 +0800</lastBuildDate><atom:link href="https://dyhes.github.io/tags/hadoop/index.xml" rel="self" type="application/rss+xml"/><item><title>【Big Data】Hadoop</title><link>https://dyhes.github.io/p/big-datahadoop/</link><pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate><guid>https://dyhes.github.io/p/big-datahadoop/</guid><description>&lt;h2 id="hadoop"&gt;Hadoop
&lt;/h2&gt;&lt;p&gt;Hadoop is an open-source framework developed by the Apache Software Foundation used for &lt;strong&gt;storing and processing&lt;/strong&gt; large datasets in a &lt;strong&gt;distributed&lt;/strong&gt; computing environment. It is designed to scale up from a single server to thousands of machines, each offering local computation and storage. Hadoop is built to handle massive amounts of data in a fault-tolerant and efficient manner.&lt;/p&gt;
&lt;h3 id="core-components"&gt;Core Components
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hadoop Distributed File System (HDFS):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; HDFS is designed to store large datasets reliably and to &lt;strong&gt;stream&lt;/strong&gt; those datasets to user applications at high bandwidth.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It follows a &lt;strong&gt;master-slave&lt;/strong&gt; architecture. The HDFS cluster consists of a single &lt;strong&gt;NameNode (master)&lt;/strong&gt; that manages the file system namespace and regulates access to files by clients. There are multiple &lt;strong&gt;DataNodes (slaves)&lt;/strong&gt; that manage storage attached to the nodes they run on.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance:&lt;/strong&gt; Data is &lt;strong&gt;replicated&lt;/strong&gt; across multiple nodes to ensure availability in case of hardware failure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Can scale up to thousands of nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High Throughput:&lt;/strong&gt; Optimized for high throughput of data access rather than low latency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MapReduce:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; A &lt;strong&gt;programming model&lt;/strong&gt; and &lt;strong&gt;execution engine&lt;/strong&gt; that is used for processing and generating large datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Components:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;JobTracker:&lt;/strong&gt; The &lt;strong&gt;master&lt;/strong&gt; node that manages the jobs and resources in the cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TaskTracker:&lt;/strong&gt; The &lt;strong&gt;slave&lt;/strong&gt; nodes that execute the tasks as directed by the JobTracker.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phases:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map Phase:&lt;/strong&gt; Processes input data and converts it into a set of key-value pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduce Phase:&lt;/strong&gt; Processes the intermediate key-value pairs to generate the final output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1&lt;/strong&gt; &lt;strong&gt;YARN (Yet Another Resource Negotiator):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; Manages resources in the Hadoop cluster and schedules jobs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Components:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResourceManager:&lt;/strong&gt; The master that controls and allocates resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeManager:&lt;/strong&gt; Manages resources and monitoring on the individual nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplicationMaster:&lt;/strong&gt; Manages the lifecycle of applications running on YARN.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hadoop Common:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; Provides common utilities and libraries that support the other Hadoop components. It includes the necessary Java libraries and files needed to start Hadoop.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ecosystem-and-tools"&gt;Ecosystem and Tools
&lt;/h3&gt;&lt;p&gt;Hadoop has a rich ecosystem of tools and frameworks that enhance its functionality:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hive:&lt;/strong&gt; A data warehouse infrastructure that provides data summarization, query, and analysis. It uses a SQL-like language called HiveQL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pig:&lt;/strong&gt; A high-level platform for creating MapReduce programs using a language called Pig Latin.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HBase:&lt;/strong&gt; A distributed, scalable, big data store modeled after Google’s Bigtable and written in Java.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sqoop:&lt;/strong&gt; A tool designed for efficiently transferring bulk data between Hadoop and structured data stores such as relational databases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flume:&lt;/strong&gt; A service for efficiently collecting, aggregating, and moving large amounts of log data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Oozie:&lt;/strong&gt; A workflow scheduling system to manage Hadoop jobs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zookeeper:&lt;/strong&gt; A centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spark:&lt;/strong&gt; A fast and general engine for large-scale data processing that can run on Hadoop clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="key-features"&gt;Key Features
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Can scale horizontally to handle more data by adding more nodes to the cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cost-Effective:&lt;/strong&gt; Uses commodity hardware, making it a cost-effective solution for big data processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Can process structured, semi-structured, and unstructured data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance:&lt;/strong&gt; Automatically handles hardware failures by replicating data across multiple nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed Computing:&lt;/strong&gt; Processes data in parallel across multiple nodes, increasing processing speed and efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="use-cases"&gt;⠀Use Cases
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Warehousing:&lt;/strong&gt; Companies use Hadoop for storing vast amounts of structured and unstructured data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Log Processing:&lt;/strong&gt; Hadoop is used to analyze and process server logs for insights and troubleshooting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recommendation Systems:&lt;/strong&gt; Used by companies like Netflix and Amazon for building recommendation engines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Archival:&lt;/strong&gt; Storing large volumes of historical data for compliance and analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="spark"&gt;Spark
&lt;/h2&gt;&lt;p&gt;Apache Spark is an open-source, distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at UC Berkeley’s AMPLab, Spark offers a fast and general-purpose cluster-computing framework. It &lt;strong&gt;extends the MapReduce model&lt;/strong&gt; to support more types of computations and &lt;strong&gt;optimizes performance&lt;/strong&gt; by keeping data in memory whenever possible.&lt;/p&gt;
&lt;h3 id="key-features-1"&gt;Key Features
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Spark &lt;strong&gt;processes data in memory&lt;/strong&gt;, which significantly reduces the time to read and write to disk. This makes Spark up to 100 times faster than Hadoop MapReduce for certain applications.&lt;/li&gt;
&lt;li&gt;The DAG (Directed Acyclic Graph) execution engine optimizes task execution by breaking down jobs into stages and tasks, which can be executed in parallel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Spark provides high-level APIs in Java, Scala, Python, and R. This versatility allows developers to use the language they are most comfortable with.&lt;/li&gt;
&lt;li&gt;The interactive shell provided by Spark allows for real-time data analysis and debugging.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compatibility:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Spark is compatible with Hadoop’s HDFS and YARN, allowing it to be deployed on existing Hadoop clusters.&lt;/li&gt;
&lt;li&gt;It also supports a wide range of data sources, including HBase, Cassandra, Kafka, and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>