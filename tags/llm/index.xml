<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on 飞鸿踏雪泥</title><link>https://dyhes.github.io/tags/llm/</link><description>Recent content in LLM on 飞鸿踏雪泥</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 30 Sep 2025 19:46:00 +0800</lastBuildDate><atom:link href="https://dyhes.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>【Shopee】LLM Knowledge</title><link>https://dyhes.github.io/p/shopeellm-knowledge/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://dyhes.github.io/p/shopeellm-knowledge/</guid><description>&lt;h2 id="context-length"&gt;Context Length
&lt;/h2&gt;&lt;p&gt;大模型的 &lt;strong&gt;Context Length&lt;/strong&gt;（上下文长度）是指模型在单次推理过程中能够处理的所有信息的总容量上限，通常以 &lt;strong&gt;Token&lt;/strong&gt;（文本的最小处理单位）来衡量。它就像模型的一次性“工作记忆区”，决定了模型能同时看到多少内容来生成回复。&lt;/p&gt;
&lt;p&gt;下面这个表格汇总了上下文长度的核心组成部分，方便你快速了解：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;组成部位&lt;/th&gt;
&lt;th&gt;包含内容&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;用户输入部分&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;当前的提问、指令&lt;/td&gt;
&lt;td&gt;即你本次向模型提出的问题或要求&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;多轮对话的历史记录 (之前的问答交替)&lt;/td&gt;
&lt;td&gt;为了让模型理解上下文，系统会自动将之前的对话内容拼接起来传给模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;上传的文档、提供的参考资料或附加说明&lt;/td&gt;
&lt;td&gt;例如让模型分析你上传的PDF、TXT等文件内容&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;模型输出部分&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;模型根据上述所有输入内容，正在生成的回答或续写内容&lt;/td&gt;
&lt;td&gt;模型产生的新的文本输出&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;为了更直观地理解这些部分如何共同构成一次完整的模型交互，你可以参考下面的流程图，它展示了上下文长度在单次推理中的动态使用过程：&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;flowchart TD
A[用户发起一次请求] --&amp;gt; B[构成本次输入的Token&amp;lt;br&amp;gt;（当前问题 + 历史对话 + 上传文档）]
A --&amp;gt; C[模型生成输出的Token&amp;lt;br&amp;gt;（正在产生的回答）]
B --&amp;gt; D[输入Token计数]
C --&amp;gt; E[输出Token计数]
D --&amp;gt; F[输入Token数 + 输出Token数]
E --&amp;gt; F
F --&amp;gt; G{总计 ≤ 上下文长度限制？}
G -- 是 --&amp;gt; H[✅ 处理成功&amp;lt;br&amp;gt;模型正常生成回答]
G -- 否 --&amp;gt; I[❌ 处理失败&amp;lt;br&amp;gt;输入被截断或输出被中断]
H --&amp;gt; J[进入下一轮对话循环]
I --&amp;gt; J
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从图表中可以看出，&lt;strong&gt;用户输入&lt;/strong&gt;和&lt;strong&gt;模型输出&lt;/strong&gt;共同消耗着有限的上下文长度预算。&lt;/p&gt;
&lt;h3 id="-关于token的简单说明"&gt;💡 关于Token的简单说明
&lt;/h3&gt;&lt;p&gt;Token是模型处理文本的基本单位，理解它有助于你更好地把握上下文长度的消耗。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于&lt;strong&gt;英文&lt;/strong&gt;，一个单词通常约为1个Token（如 &amp;ldquo;hello&amp;rdquo;），但长词可能被拆分（如 &amp;ldquo;chatGPT&amp;rdquo; 可能为2个Token）。&lt;/li&gt;
&lt;li&gt;对于&lt;strong&gt;中文&lt;/strong&gt;，一个汉字大致相当于1个Token（如“你好”约为2个Tokens）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标点符号、数字和空格&lt;/strong&gt;等也都计算在内。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;粗略估算时，可以认为 &lt;strong&gt;1个Token约等于1个汉字&lt;/strong&gt;的长度。&lt;/p&gt;
&lt;h3 id="-重要注意事项"&gt;⚠️ 重要注意事项
&lt;/h3&gt;&lt;p&gt;了解以下几点，能帮助你更有效地使用模型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;硬性限制与动态消耗&lt;/strong&gt;：上下文长度是一个&lt;strong&gt;硬性上限&lt;/strong&gt;。如图表所示，单次交互中“输入+输出”的Token总数绝不能超过此限制，否则最早的信息会被丢弃（截断），以确保处理正常进行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多轮对话的累积效应&lt;/strong&gt;：在多轮对话中，为了维持对话连贯性，&lt;strong&gt;之前所有轮次的对话历史（你的问题和模型的回答）都会作为新的输入信息的一部分，被再次传递给模型&lt;/strong&gt;。这意味着对话轮数越多，消耗的上下文长度就越多，可用的剩余空间就越少。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最大输出长度的约束&lt;/strong&gt;：除了总长度限制，模型通常还有一个&lt;strong&gt;最大输出长度&lt;/strong&gt;（Max Output Tokens）的限制，即单次回复能生成的最大Token数。例如，即使上下文长度有128K，但模型单次回答可能被限制为只能输出4K Token。因此，实际可用输入长度 ≈ 上下文总长度 - 最大输出长度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“失忆”现象的根源&lt;/strong&gt;：当长对话累积的Token数超过模型的上下文长度限制时，系统会&lt;strong&gt;丢弃最早的信息&lt;/strong&gt;（通常是对话开头部分）以容纳新输入。这就是为什么模型在长对话后可能“忘记”很久之前讨论过的内容，这并非模型本身故障，而是工程上的处理策略。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="总结与应用建议"&gt;总结与应用建议
&lt;/h3&gt;&lt;p&gt;理解上下文长度的组成和限制，能帮助你更高效地与大型语言模型交互：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;处理长文档&lt;/strong&gt;：若需分析长文档，可先提取关键章节或分段提交，以避免耗尽上下文空间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;管理长对话&lt;/strong&gt;：在长时间、多轮对话中，对于重要的前期结论或信息，可有意识地&lt;strong&gt;在后续提问中重申或提及&lt;/strong&gt;，以防其因截断而被“遗忘”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;控制输出长度&lt;/strong&gt;：若需要模型生成非常长的内容（如长篇文章、报告），而它的单次输出长度有限，可以&lt;strong&gt;引导它分步骤、分章节地生成&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;希望这些信息能帮助你更好地理解和使用大模型。&lt;/p&gt;
&lt;h2 id="常用上下文"&gt;常用上下文
&lt;/h2&gt;&lt;p&gt;了解当前主流大模型的上下文长度（Context Length）对于选择合适的模型至关重要。上下文长度决定了模型单次处理信息的容量，直接影响到处理长文档、维持长对话和复杂推理的能力。以下是截至2025年的流行大模型及其上下文长度信息，供你参考。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型系列&lt;/th&gt;
&lt;th&gt;模型名称 (或版本)&lt;/th&gt;
&lt;th&gt;上下文长度 (Tokens)&lt;/th&gt;
&lt;th&gt;备注/特点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;国外模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI&lt;/td&gt;
&lt;td&gt;GPT-4 (8K版)&lt;/td&gt;
&lt;td&gt;8,192&lt;/td&gt;
&lt;td&gt;支持约8k tokens输入输出&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;GPT-4 (32K版)&lt;/td&gt;
&lt;td&gt;32,768&lt;/td&gt;
&lt;td&gt;单次可处理约2.5万字中文或英文&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;GPT-5&lt;/td&gt;
&lt;td&gt;1,000,000+&lt;/td&gt;
&lt;td&gt;超长上下文处理能力达1M+ tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Anthropic&lt;/td&gt;
&lt;td&gt;Claude 4&lt;/td&gt;
&lt;td&gt;1,000,000 (1000k)&lt;/td&gt;
&lt;td&gt;超低幻觉，适用于法律、医疗等高风险领域&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google DeepMind&lt;/td&gt;
&lt;td&gt;Gemini 系列标准版&lt;/td&gt;
&lt;td&gt;~32,768&lt;/td&gt;
&lt;td&gt;性能接近GPT-4，未明确公布具体数值&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Gemini 1.5 Flash&lt;/td&gt;
&lt;td&gt;1,000,000&lt;/td&gt;
&lt;td&gt;支持百万token上下文&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Gemini 1.5 Pro&lt;/td&gt;
&lt;td&gt;2,000,000&lt;/td&gt;
&lt;td&gt;支持两百万token上下文&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;
&lt;td&gt;1,000,000&lt;/td&gt;
&lt;td&gt;液态神经网络架构，响应延迟低&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Meta (Facebook)&lt;/td&gt;
&lt;td&gt;Llama 2&lt;/td&gt;
&lt;td&gt;4,096&lt;/td&gt;
&lt;td&gt;相比Llama 1的2048 tokens翻倍&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Llama 4&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;提供万亿参数版本，支持100+语言，手机端部署能力强&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mistral AI&lt;/td&gt;
&lt;td&gt;Mistral-Next&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;混合专家（MoE）架构，效率高&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;国内模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;百度 (Baidu)&lt;/td&gt;
&lt;td&gt;文心大模型 5.0 (ERNIE 5.0)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;产业级多模态能力，融合行业数据库&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;阿里巴巴 (Alibaba)&lt;/td&gt;
&lt;td&gt;通义千问3.0 (Qwen3)&lt;/td&gt;
&lt;td&gt;500,000&lt;/td&gt;
&lt;td&gt;超长文本支持500k tokens，电商优化能力突出&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;通义千问-Max&lt;/td&gt;
&lt;td&gt;32,000~1,000,000&lt;/td&gt;
&lt;td&gt;适合复杂任务，推理能力最强&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;通义千问-Plus&lt;/td&gt;
&lt;td&gt;1,000,000&lt;/td&gt;
&lt;td&gt;性能均衡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;通义千问-Flash&lt;/td&gt;
&lt;td&gt;1,000,000&lt;/td&gt;
&lt;td&gt;适合简单任务，速度快、成本低&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;腾讯 (Tencent)&lt;/td&gt;
&lt;td&gt;混元大模型3.0 (Hunyuan 3.0)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;游戏NPC智能化，微信深度整合&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;华为 (Huawei)&lt;/td&gt;
&lt;td&gt;盘古大模型4.0&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;行业专用小模型，端侧AI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;科大讯飞 (iFLYTEK)&lt;/td&gt;
&lt;td&gt;讯飞星火V4.0 Turbo&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;中英双语对齐优化，支持202种方言识别&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度求索 (DeepSeek)&lt;/td&gt;
&lt;td&gt;DeepSeek-V3&lt;/td&gt;
&lt;td&gt;128,000&lt;/td&gt;
&lt;td&gt;混合推理架构，数学与代码能力突出&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;月之暗面 (Moonshot)&lt;/td&gt;
&lt;td&gt;Kimi+&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;强调1000万字上下文（全球最长文本处理能力之一）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;智谱AI (Zhipu AI)&lt;/td&gt;
&lt;td&gt;GLM-5&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;中英双语对齐，开源版本增强&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;💡 重要说明：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Token换算参考&lt;/strong&gt;：通常，1个Token约等于1-2个汉字（中文）或0.75个单词（英文）。例如，32K tokens大约对应2.4万至4.8万个汉字。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实际限制&lt;/strong&gt;：模型声称的上下文长度是&lt;strong&gt;理论最大值&lt;/strong&gt;。实际使用时，&lt;strong&gt;输入（你的问题+上下文）和输出（模型的回答）的Token数之和&lt;/strong&gt;不能超过此限制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术发展&lt;/strong&gt;：上下文长度是当前大模型技术竞争的一个焦点，数值更新很快。部分模型（如GPT-5、Claude 4、Gemini 1.5/2.5 Pro）已支持&lt;strong&gt;百万级（1M+）甚至两百万（2M）Token&lt;/strong&gt;，使其能够处理整本书、长篇论文或极其复杂的多轮对话。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择建议&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;处理日常问答、短文总结：&lt;strong&gt;8K-32K&lt;/strong&gt; 的模型通常足够。&lt;/li&gt;
&lt;li&gt;分析长篇报告、技术文档、代码库：建议选择 &lt;strong&gt;100K以上&lt;/strong&gt; 的模型。&lt;/li&gt;
&lt;li&gt;进行超长文献解析、书籍内容分析或极其复杂的多轮对话：应考虑 &lt;strong&gt;500K至1M+&lt;/strong&gt; 的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;希望这份列表能帮助你更好地了解当前大模型的上下文长度能力。如果你有特定的应用场景，可以据此选择最适合的模型。&lt;/p&gt;</description></item><item><title>【Shopee】Local LLM</title><link>https://dyhes.github.io/p/shopeelocal-llm/</link><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate><guid>https://dyhes.github.io/p/shopeelocal-llm/</guid><description>&lt;h2 id="gguf"&gt;GGUF
&lt;/h2&gt;&lt;p&gt;GGUF（GPT-Generated Unified Format）是一种专为&lt;strong&gt;大型语言模型（LLM）本地化部署&lt;/strong&gt;设计的二进制文件格式，由 &lt;code&gt;llama.cpp&lt;/code&gt; 项目的创始人 Georgi Gerganov 提出，旨在解决大模型在存储、加载速度和跨平台兼容性上的挑战。以下是其核心特性与应用场景的详细解析：&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id="-一核心设计目标"&gt;🔧 &lt;strong&gt;一、核心设计目标&lt;/strong&gt;
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;高效存储与加载&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;内存映射（mmap）支持&lt;/strong&gt;：GGUF 文件可直接映射到内存，无需完全加载到 RAM，大幅减少启动时间和内存占用（尤其适合资源受限设备）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单文件部署&lt;/strong&gt;：模型权重、配置和分词器信息集成在单一文件中，简化分发与使用流程。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;量化支持&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;支持多种量化类型（如 &lt;code&gt;Q4_K_M&lt;/code&gt;、&lt;code&gt;Q5_K_M&lt;/code&gt;），通过降低权重精度（如 4-bit/8-bit）压缩模型体积，平衡性能与资源消耗。&lt;/li&gt;
&lt;li&gt;例如：&lt;code&gt;Q4_K_M&lt;/code&gt; 表示 &lt;strong&gt;4-bit 主量化&lt;/strong&gt; + &lt;strong&gt;分块优化（K）&lt;/strong&gt; + &lt;strong&gt;中等精度（M）&lt;/strong&gt;，适合 16GB RAM 设备。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;跨平台兼容性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;支持 CPU/GPU 加速（如 CUDA、Metal），可在 Windows/Linux/macOS 及边缘设备运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="-二文件结构"&gt;🧬 &lt;strong&gt;二、文件结构&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;GGUF 文件包含三个核心部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文件头（Header）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标识文件类型（&lt;code&gt;GGUF&lt;/code&gt;）、版本号和张量数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;元数据段（Metadata）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以键值对存储模型架构、训练参数、分词器配置等关键信息（如 &lt;code&gt;llama.context_length&lt;/code&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;张量数据段（Tensors）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储量化后的权重数据，按分块（Block）组织以优化读取效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id="-三命名规范"&gt;📛 &lt;strong&gt;三、命名规范&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;GGUF 文件名遵循结构化规则，便于快速识别模型属性：
​**&lt;code&gt;&amp;lt;BaseName&amp;gt;-&amp;lt;SizeLabel&amp;gt;-&amp;lt;FineTune&amp;gt;-&amp;lt;Version&amp;gt;-&amp;lt;Encoding&amp;gt;-&amp;lt;Type&amp;gt;-&amp;lt;Shard&amp;gt;.gguf&lt;/code&gt;**​&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;p&gt;：&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;Mixtral-8x7B-v0.1-Q4_K_M.gguf
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Mixtral&lt;/code&gt;：基础模型架构&lt;/li&gt;
&lt;li&gt;&lt;code&gt;8x7B&lt;/code&gt;：8 个专家模块，共 70 亿参数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Q4_K_M&lt;/code&gt;：4-bit 量化 + 分块优化 + 中等精度。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id="-四技术优势"&gt;⚡️ &lt;strong&gt;四、技术优势&lt;/strong&gt;
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;性能优化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;实测中，GGUF 格式的推理速度比 Hugging Face 格式快 &lt;strong&gt;7.57 倍&lt;/strong&gt;，内存占用仅为其 &lt;strong&gt;4.65%&lt;/strong&gt;（如 Yuan2.0-2B 模型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;灵活扩展性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;元数据段支持动态添加新字段，确保新功能不破坏旧版本兼容性。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;量化分级策略&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;提供多级量化选项（如&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;Q2_K
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;到&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;Q8_K
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;），用户可根据硬件选择：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Q4_K_M&lt;/code&gt;：通用场景（推荐）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Q5_K_S&lt;/code&gt;：低内存设备&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Q8_0&lt;/code&gt;：接近原精度（需高性能硬件）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id="-五与其他格式对比"&gt;🆚 &lt;strong&gt;五、与其他格式对比&lt;/strong&gt;
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;格式&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GGUF&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;CPU 优化、支持量化、低内存占用&lt;/td&gt;
&lt;td&gt;本地部署（Ollama/llama.cpp）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;PyTorch (.pt)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;原生训练格式、未量化、依赖 GPU&lt;/td&gt;
&lt;td&gt;模型训练/微调&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Safetensors&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;安全序列化、需完整加载到内存&lt;/td&gt;
&lt;td&gt;云端 GPU 推理&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ONNX&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;跨框架通用、硬件加速支持有限&lt;/td&gt;
&lt;td&gt;跨平台部署&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id="-六典型应用场景"&gt;💻 &lt;strong&gt;六、典型应用场景&lt;/strong&gt;
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本地 LLM 部署&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过 &lt;code&gt;llama.cpp&lt;/code&gt; 或 &lt;code&gt;Ollama&lt;/code&gt; 在个人电脑运行百亿级模型（如 LLaMA-3、Qwen2）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;边缘计算&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在树莓派等设备部署轻量化模型（如 0.6B 参数的 &lt;code&gt;Qwen3-Embedding&lt;/code&gt;）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;RAG 系统&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结合嵌入模型（如 &lt;code&gt;Qwen3-Embedding-4B&lt;/code&gt;）和重排序模型（如 &lt;code&gt;Qwen3-Reranker&lt;/code&gt;）构建本地知识库。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id="-七模型转换方法"&gt;🔄 &lt;strong&gt;七、模型转换方法&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;若需将其他格式（如 Hugging Face 的 &lt;code&gt;safetensors&lt;/code&gt;）转为 GGUF：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;工具链&lt;/strong&gt;：使用 &lt;code&gt;llama.cpp&lt;/code&gt; 的 &lt;code&gt;convert-hf-to-gguf.py&lt;/code&gt; 脚本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;量化步骤&lt;/p&gt;
&lt;p&gt;：&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;python convert-hf-to-gguf.py &amp;lt;模型目录&amp;gt; --outfile &amp;lt;输出文件.gguf&amp;gt;
./quantize &amp;lt;输入.gguf&amp;gt; &amp;lt;输出_量化版.gguf&amp;gt; Q4_K_M
&lt;/code&gt;&lt;/pre&gt;&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;在线转换&lt;/strong&gt;：通过 Hugging Face 的 &lt;code&gt;gguf-my-repo&lt;/code&gt; 空间一键转换社区模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="-总结"&gt;💎 &lt;strong&gt;总结&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;GGUF 通过&lt;strong&gt;高效存储设计&lt;/strong&gt;、&lt;strong&gt;灵活量化策略&lt;/strong&gt;和&lt;strong&gt;跨平台兼容性&lt;/strong&gt;，成为本地大模型部署的&lt;strong&gt;事实标准&lt;/strong&gt;。其单文件特性大幅降低了开发者使用门槛，而分块量化技术则让百亿模型在消费级硬件上流畅运行成为可能。&lt;/p&gt;
&lt;h2 id="q4_k_m"&gt;Q4_K_M
&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Q4_K_M&lt;/code&gt; 是 &lt;strong&gt;GGUF 模型量化格式&lt;/strong&gt;中的一种特定量化级别，专为在资源受限设备（如普通 PC 或移动设备）上高效运行大语言模型而设计。以下是其核心含义的详细解析：&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id="-一命名拆解q4_k_m-的结构含义"&gt;🔧 一、命名拆解：&lt;code&gt;Q4_K_M&lt;/code&gt; 的结构含义
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;Q4&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表示主量化位宽为 &lt;strong&gt;4 比特&lt;/strong&gt;（即每个权重平均用 4 位存储），显著压缩模型体积（例如 70 亿参数模型可压缩至约 4GB）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;K&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;指&lt;/p&gt;
&lt;p&gt;分块量化（Block-wise Quantization）&lt;/p&gt;
&lt;p&gt;技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将权重矩阵划分为多个小块（如每组 32~64 个权重）。&lt;/li&gt;
&lt;li&gt;每个块独立计算&lt;strong&gt;缩放因子（Scale）&lt;/strong&gt; 和 &lt;strong&gt;零点（Zero Point）&lt;/strong&gt;，减少整体量化误差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;M&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;表示&lt;/p&gt;
&lt;p&gt;混合精度优化级别为中等（Medium）&lt;/p&gt;
&lt;p&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对关键层（如注意力机制中的 &lt;code&gt;attention.wv&lt;/code&gt; 和前馈网络 &lt;code&gt;feed_forward.w2&lt;/code&gt;）使用更高精度（如 6 比特），其他层用 4 比特。&lt;/li&gt;
&lt;li&gt;平衡性能与资源占用，是 &lt;strong&gt;推荐默认选择&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="-二性能特点平衡体积速度与精度"&gt;⚖️ 二、性能特点：平衡体积、速度与精度
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;特性&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Q4_K_M 表现&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;对比其他量化级别&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;内存占用&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;比原模型小 70%（如 7B 模型降至 ~4GB）&lt;/td&gt;
&lt;td&gt;比 &lt;code&gt;Q5_K_M&lt;/code&gt; 节省 15% 内存&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;推理速度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;中等（CPU 上约 20 token/s）&lt;/td&gt;
&lt;td&gt;比 &lt;code&gt;Q4_0&lt;/code&gt; 略慢，但精度更高&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;精度保持&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;损失 &amp;lt;5%（在 WikiText 测试集上 PPL 困惑度接近原模型）&lt;/td&gt;
&lt;td&gt;显著优于 &lt;code&gt;Q4_0&lt;/code&gt;，略逊于 &lt;code&gt;Q5_K_M&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;通用任务（文本生成、问答）、中低配置硬件（如 16GB RAM 的笔记本）&lt;/td&gt;
&lt;td&gt;高配置设备可选 &lt;code&gt;Q5_K_M&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id="-三技术原理如何实现高效压缩"&gt;🛠️ 三、技术原理：如何实现高效压缩
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;混合精度策略&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;动态识别重要权重（如注意力层的部分张量），分配更高比特（6-bit），非关键部分用 4-bit，最大化保留模型能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;分块量化优化&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个块独立量化，避免全局误差累积。例如：
&lt;ul&gt;
&lt;li&gt;块内权重共享缩放因子，减少存储开销。&lt;/li&gt;
&lt;li&gt;超块（Super Block）结构进一步细化精度控制。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;性能对比实例&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在&lt;/p&gt;
&lt;p&gt;LLaMA-7B&lt;/p&gt;
&lt;p&gt;测试中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Q4_K_M&lt;/code&gt; 的 PPL（困惑度）为 &lt;strong&gt;6.8&lt;/strong&gt;，接近原模型（6.5）。&lt;/li&gt;
&lt;li&gt;而 &lt;code&gt;Q4_0&lt;/code&gt; 的 PPL 为 &lt;strong&gt;7.5&lt;/strong&gt;，误差更大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="-四实际应用建议"&gt;💡 四、实际应用建议
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;推荐场景&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;普通 PC 或笔记本（16GB RAM）部署大模型（如通义千问 4B/7B）。&lt;/li&gt;
&lt;li&gt;需要平衡速度与质量的 RAG、代码生成等任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;替代方案&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源极度紧张（&amp;lt;8GB RAM）→ 选 &lt;code&gt;Q4_0&lt;/code&gt; 或 &lt;code&gt;Q3_K_M&lt;/code&gt;（牺牲精度换速度）。&lt;/li&gt;
&lt;li&gt;追求极致质量（如学术研究）→ 选 &lt;code&gt;Q5_K_M&lt;/code&gt; 或 &lt;code&gt;Q6_K&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;使用示例（Ollama）&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;ollama run qwen:7b-q4_k_m # 拉取通义千问7B的Q4_K_M量化版
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="-五注意事项"&gt;⚠️ 五、注意事项
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;量化本质有损&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;极端任务（如高精度数学计算）可能受影响，建议优先测试业务场景。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;框架兼容性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;仅适用于 &lt;code&gt;llama.cpp&lt;/code&gt; 或 &lt;code&gt;Ollama&lt;/code&gt; 等支持 GGUF 的推理框架，&lt;strong&gt;不兼容 PyTorch 或 vLLM&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="-总结-1"&gt;💎 总结
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Q4_K_M = 4 比特主量化 + 分块优化 + 中等混合精度&lt;/strong&gt;，是 &lt;strong&gt;性价比最优&lt;/strong&gt; 的量化选择之一——在压缩率、速度和质量间取得平衡，适合大多数本地化部署需求。若你正在普通配置设备上运行 7B~13B 级模型，它通常是最稳妥的起点 🚀。&lt;/p&gt;</description></item></channel></rss>