<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Introduction In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance.\u00a0The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores. At its core are three key abstractions — a hierarchy of thread groups, shared memories, and barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions. Each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. "><title>【CUDA】Programming Guide</title><link rel=canonical href=https://dyhes.github.io/p/cudaprogramming-guide/><link rel=stylesheet href=/scss/style.min.f7091bff8043bd3e53b22be6c05dd86b506e8dec4d0d75d249d2dfb0fe074a46.css><meta property='og:title' content="【CUDA】Programming Guide"><meta property='og:description' content="Introduction In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance.\u00a0The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores. At its core are three key abstractions — a hierarchy of thread groups, shared memories, and barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions. Each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. "><meta property='og:url' content='https://dyhes.github.io/p/cudaprogramming-guide/'><meta property='og:site_name' content='飞鸿踏雪泥'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='CUDA'><meta property='article:published_time' content='2025-06-13T00:00:00+00:00'><meta property='article:modified_time' content='2025-10-22T16:27:30+08:00'><meta name=twitter:title content="【CUDA】Programming Guide"><meta name=twitter:description content="Introduction In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance.\u00a0The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores. At its core are three key abstractions — a hierarchy of thread groups, shared memories, and barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions. Each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. "><link rel="shortcut icon" href=/github.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_b567f26f71c49c33.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>飞鸿踏雪泥</a></h1><h2 class=site-description>没有记录，就没有发生</h2></div></header><ol class=menu-social><li><a href=https://leetcode.cn/u/dyhes/ target=_blank title=LeetCode rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 13h7.5"/><path d="M9.424 7.268l4.999-4.999"/><path d="M16.633 16.644l-2.402 2.415a3.189 3.189.0 01-4.524.0l-3.77-3.787a3.223 3.223.0 010-4.544l3.77-3.787a3.189 3.189.0 014.524.0l2.302 2.313"/></svg></a></li><li><a href=https://github.com/dyhes target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:dyheslin@gmail.com target=_blank title=Gmail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-gmail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li><li><a href=mailto:1325574784@qq.com target=_blank title=Mail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M13 19H5a2 2 0 01-2-2V7a2 2 0 012-2h14a2 2 0 012 2v5.5"/><path d="M3 7l9 6 9-6"/><path d="M19 16l-2 3h4l-2 3"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/categories/><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#programming-model>Programming Model</a><ol><li><a href=#kernel>Kernel</a></li><li><a href=#thread-hierarchy>Thread Hierarchy</a></li><li><a href=#memory-hierarchy>Memory Hierarchy</a></li><li><a href=#heterogeneous-programming>Heterogeneous Programming</a></li><li><a href=#asynchronous-simt-programming-model>Asynchronous SIMT Programming Model</a></li></ol></li><li><a href=#programming-interface>Programming Interface</a><ol><li><a href=#nvcc>NVCC</a><ol><li><a href=#offline-compilation>Offline Compilation</a></li><li><a href=#just-in-time-compilation>Just-in-Time Compilation</a></li><li><a href=#compatibility>Compatibility</a></li></ol></li><li><a href=#runtime>Runtime</a><ol><li><a href=#initialization>Initialization</a></li><li><a href=#device-memory>Device Memory</a></li><li><a href=#shared-memory>Shared Memory</a></li><li><a href=#distributed-shared-memory>Distributed Shared Memory</a></li><li><a href=#page-locked-host-memory>Page-Locked Host Memory</a></li><li><a href=#memory-synchronization>Memory Synchronization</a></li></ol></li><li><a href=#asynchronous-concurrent-execution>Asynchronous Concurrent Execution</a></li><li><a href=#stream>Stream</a><ol><li><a href=#default-stream>default stream</a></li><li><a href=#explicit-synchronization>explicit synchronization</a></li><li><a href=#implicit-synchronization>implicit synchronization</a></li><li><a href=#host-functions><strong>Host Functions</strong></a></li><li><a href=#priority>Priority</a></li><li><a href=#dependent-launch>Dependent Launch</a></li></ol></li><li><a href=#events>Events</a></li><li><a href=#error-checking>Error Checking</a></li><li><a href=#compute-mode>Compute Mode</a></li></ol></li><li><a href=#hardware-implementation>Hardware Implementation</a><ol><li><a href=#simt>SIMT</a></li><li><a href=#hardware-multithreading>Hardware Multithreading</a></li></ol></li><li><a href=#cooperative-groups>Cooperative Groups</a><ol><li><a href=#group-type>Group Type</a><ol><li><a href=#implicit-groups>Implicit Groups</a></li><li><a href=#explicit-groups>Explicit Groups</a></li></ol></li><li><a href=#group-partitioning>Group Partitioning</a></li><li><a href=#group-collectives>Group Collectives</a><ol><li><a href=#memcpy_async>memcpy_async</a></li></ol></li><li><a href=#multi-device>Multi-Device</a></li></ol></li><li><a href=#c-language-extensions>C++ Language Extensions</a><ol><li><a href=#function-execution-space-specifier>Function Execution Space Specifier</a><ol><li><a href=#__global__>__global__</a></li><li><a href=#__device__>__device__</a></li><li><a href=#__host__>__host__ </a></li><li><a href=#inline>inline</a></li></ol></li><li><a href=#variable-memory-space-specifier>Variable Memory Space Specifier</a><ol><li><a href=#__device__-1>__device__</a></li><li><a href=#__shared__>__shared__</a></li></ol></li><li><a href=#dim3>dim3</a></li><li><a href=#built-in-variables>Built-in Variables</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nutrition/ style=background-color:#93b5cf;color:>积雪粮</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/cudaprogramming-guide/>【CUDA】Programming Guide</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 13, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>43 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction>Introduction</h2><p>In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance. 
<img src=https://i.ibb.co/7xgqKH7d/image.png loading=lazy>
The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores.
At its core are three key abstractions — a hierarchy of <strong>thread groups</strong>, <strong>shared memories</strong>, and <strong>barrier synchronization</strong> — that are simply exposed to the programmer as a minimal set of language extensions.
Each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors.
<img src=https://i.ibb.co/hxDXmRFY/image-2.png loading=lazy></p><h2 id=programming-model>Programming Model</h2><h3 id=kernel>Kernel</h3><p>CUDA C++ extends C++ by allowing the programmer to define C++ functions, called <em>kernels</em>, that, when called, are executed N times in parallel by N different <em>CUDA threads</em>, as opposed to only once like regular C++ functions.
A kernel is defined using the <strong>global</strong> declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new &#171;&lt;&mldr;&#187;><em>execution configuration</em> syntax (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration target=_blank rel=noopener>Execution Configuration</a>). Each thread that executes the kernel is given a <strong>unique <em>thread ID</em></strong> that is accessible within the kernel through built-in variables.</p><h3 id=thread-hierarchy>Thread Hierarchy</h3><p>For convenience, threadIdx is a <strong>3-component vector</strong>, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional <em>thread index</em>, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a <em>thread block</em>.
The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the <strong>same</strong>; for a two-dimensional block of size <em>(Dx, Dy)</em>, the thread ID of a thread of index <em>(x, y)</em> is <em>(<strong>x + y Dx</strong>)</em>; for a three-dimensional block of size <em>(Dx, Dy, Dz)</em>, the thread ID of a thread of index <em>(x, y, z)</em> is <em>(<strong>x + y Dx + z Dx Dy</strong>)</em>.
There is a limit to the number of threads per block, since all threads of a block are expected to <strong>reside</strong> on the same streaming multiprocessor core and must <strong>share</strong> the limited memory resources of that core. On current GPUs, a thread block may contain <strong>up to 1024</strong> threads.
Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional <em>grid</em> of thread blocks.
<img src=https://i.ibb.co/tpTsSt42/image-3.png loading=lazy>
The number of threads per block and the number of blocks per grid specified in the &#171;&lt;&mldr;&#187;> syntax can be of type int or dim3.
Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable.
A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. 
Thread blocks are required to execute <strong>independently</strong>. It must be possible to execute blocks in any order, <strong>in parallel or in series</strong>. This independence requirement allows thread blocks to be scheduled in any order and across any number of cores.
Threads within a block can <strong>cooperate</strong> by sharing data through some <em>shared memory</em> and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the <strong>__syncthreads()</strong> intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed. <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory target=_blank rel=noopener>Shared Memory</a> gives an example of using shared memory. In addition to __syncthreads(), the <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups target=_blank rel=noopener>Cooperative Groups API</a> provides a rich set of thread-synchronization primitives.
With the introduction of NVIDIA <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-9-0 target=_blank rel=noopener>Compute Capability 9.0</a>, the CUDA programming model introduces an <strong>optional</strong> level of hierarchy called Thread Block Clusters that are made up of thread blocks. Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU Processing Cluster (GPC) in the GPU.
Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension grid of thread block clusters. The number of thread blocks in a cluster can be user-defined, and <strong>a maximum of 8 thread blocks</strong> in a cluster is supported as a portable cluster size in CUDA. Note that on GPU hardware or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly. Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is <strong>architecture-specific</strong> and can be queried using the <strong><code>cudaOccupancyMaxPotentialClusterSize</code></strong> API.
<img src=https://i.ibb.co/G3HT4zGK/image-4.png loading=lazy></p><blockquote><p>In a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for <strong>compatibility</strong> purposes. The rank of a block in a cluster can be found using the <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cluster-group-cg target=_blank rel=noopener>Cluster Group</a> API.
A thread block cluster can be enabled in a kernel either using a <strong>compile-time</strong> kernel attribute using <strong>cluster_dims</strong>(X,Y,Z) or using the CUDA kernel launch API <code>cudaLaunchKernelEx</code>.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Kernel definition
</span></span></span><span class=line><span class=cl><span class=c1>// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>__cluster_dims__</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=n>cluster_kernel</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>input</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=o>*</span><span class=n>input</span><span class=p>,</span> <span class=o>*</span><span class=n>output</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Kernel invocation with compile time cluster size
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>dim3</span> <span class=n>threadsPerBlock</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>16</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>dim3</span> <span class=n>numBlocks</span><span class=p>(</span><span class=n>N</span> <span class=o>/</span> <span class=n>threadsPerBlock</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>N</span> <span class=o>/</span> <span class=n>threadsPerBlock</span><span class=p>.</span><span class=n>y</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// The grid dimension is not affected by cluster launch, and is still enumerated
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// using number of blocks.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// The grid dimension must be a multiple of cluster size.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>cluster_kernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>numBlocks</span><span class=p>,</span> <span class=n>threadsPerBlock</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>input</span><span class=p>,</span> <span class=n>output</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Kernel definition
</span></span></span><span class=line><span class=cl><span class=c1>// No compile time attribute attached to the kernel
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>cluster_kernel</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>input</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=o>*</span><span class=n>input</span><span class=p>,</span> <span class=o>*</span><span class=n>output</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>dim3</span> <span class=n>threadsPerBlock</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>16</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>dim3</span> <span class=n>numBlocks</span><span class=p>(</span><span class=n>N</span> <span class=o>/</span> <span class=n>threadsPerBlock</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>N</span> <span class=o>/</span> <span class=n>threadsPerBlock</span><span class=p>.</span><span class=n>y</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Kernel invocation with runtime cluster size
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>cudaLaunchConfig_t</span> <span class=n>config</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>};</span>
</span></span><span class=line><span class=cl>        <span class=c1>// The grid dimension is not affected by cluster launch, and is still enumerated
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// using number of blocks.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// The grid dimension should be a multiple of cluster size.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>config</span><span class=p>.</span><span class=n>gridDim</span> <span class=o>=</span> <span class=n>numBlocks</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>.</span><span class=n>blockDim</span> <span class=o>=</span> <span class=n>threadsPerBlock</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>cudaLaunchAttribute</span> <span class=n>attribute</span><span class=p>[</span><span class=mi>1</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>attribute</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=n>id</span> <span class=o>=</span> <span class=n>cudaLaunchAttributeClusterDimension</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>attribute</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=n>val</span><span class=p>.</span><span class=n>clusterDim</span><span class=p>.</span><span class=n>x</span> <span class=o>=</span> <span class=mi>2</span><span class=p>;</span> <span class=c1>// Cluster size in X-dimension
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>attribute</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=n>val</span><span class=p>.</span><span class=n>clusterDim</span><span class=p>.</span><span class=n>y</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>attribute</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=n>val</span><span class=p>.</span><span class=n>clusterDim</span><span class=p>.</span><span class=n>z</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>.</span><span class=n>attrs</span> <span class=o>=</span> <span class=n>attribute</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>.</span><span class=n>numAttrs</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>cudaLaunchKernelEx</span><span class=p>(</span><span class=o>&amp;</span><span class=n>config</span><span class=p>,</span> <span class=n>cluster_kernel</span><span class=p>,</span> <span class=n>input</span><span class=p>,</span> <span class=n>output</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In GPUs with compute capability <strong>9.0</strong>, all the thread blocks in the cluster are guaranteed to be co-scheduled on a single <strong>GPU Processing Cluster (GPC)</strong> and allow thread blocks in the cluster to perform <strong>hardware-supported synchronization</strong> using the <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cluster-group-cg target=_blank rel=noopener>Cluster Group</a> API <code>cluster.sync()</code>. Cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks using <code>num_threads()</code> and <code>num_blocks()</code> API respectively. The rank of a thread or block in the cluster group can be queried using <code>dim_threads()</code> and <code>dim_blocks()</code> API respectively.
Thread blocks that belong to a cluster have access to the <strong>Distributed Shared Memory</strong>. Thread blocks in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared memory.</p><h3 id=memory-hierarchy>Memory Hierarchy</h3><p><img src=https://i.ibb.co/qMRxr4DF/image-5.png loading=lazy>
There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are <strong>optimized for different memory usages</strong>.</p><h3 id=heterogeneous-programming>Heterogeneous Programming</h3><p>The CUDA programming model also assumes that both the host and the device maintain their <strong>own</strong> separate memory spaces in DRAM, referred to as <em>host memory</em> and <em>device memory</em>, respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-interface target=_blank rel=noopener>Programming Interface</a>). This includes device memory <strong>allocation</strong> and <strong>deallocation</strong> as well as data <strong>transfer</strong> between host and device memory.
Unified Memory provides <em>managed memory</em> to <strong>bridge</strong> the host and device memory spaces. Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. This capability enables <strong>oversubscription</strong> of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device.</p><h3 id=asynchronous-simt-programming-model>Asynchronous SIMT Programming Model</h3><p>In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation. Starting with devices based on the <strong>NVIDIA Ampere GPU Architecture</strong>, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model. The asynchronous programming model defines the behavior of asynchronous operations with respect to CUDA threads.
The asynchronous programming model defines the behavior of <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#aw-barrier target=_blank rel=noopener>Asynchronous Barrier</a> for synchronization between CUDA threads. The model also explains and defines how <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies target=_blank rel=noopener>cuda::memcpy_async</a> can be used to move data asynchronously from global memory while computing in the GPU.
An asynchronous operation is defined as an operation that is <strong>initiated by a</strong> CUDA thread and is <strong>executed asynchronously as-if by another</strong> thread. In a well formed program one or more CUDA threads synchronize with the <strong>asynchronous operation</strong>. The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads.</p><blockquote><p>发起异步操作的CUDA线程无需参与该操作的同步等待过程，其他线程可以代替它完成同步
Such an asynchronous thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation. An asynchronous operation uses a synchronization object to synchronize the completion of the operation. Such a synchronization object can be explicitly managed by a user (e.g., cuda::memcpy_async) or implicitly managed within a library (e.g., cooperative_groups::memcpy_async).
A synchronization object could be a cuda::barrier or a cuda::pipeline. These objects are explained in detail in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#aw-barrier target=_blank rel=noopener>Asynchronous Barrier</a> and <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies target=_blank rel=noopener>Asynchronous Data Copies using cuda::pipeline</a>.
These synchronization objects can be used at <strong>different thread scopes</strong>. A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation. The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each.</p><div class=table-wrapper><table><thead><tr><th><strong>Thread Scope</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>cuda::thread_scope::thread_scope_thread</td><td>Only the CUDA thread which initiated asynchronous operations synchronizes.</td></tr><tr><td>cuda::thread_scope::thread_scope_block</td><td>All or any CUDA threads within the same thread block as the initiating thread synchronizes.</td></tr><tr><td>cuda::thread_scope::thread_scope_device</td><td>All or any CUDA threads in the same GPU device as the initiating thread synchronizes.</td></tr><tr><td>cuda::thread_scope::thread_scope_system</td><td>All or any CUDA or CPU threads in the same system as the initiating thread synchronizes.</td></tr><tr><td>These thread scopes are implemented as extensions to standard C++ in the <a class=link href=https://nvidia.github.io/libcudacxx/extended_api/memory_model.html#thread-scopes target=_blank rel=noopener>CUDA Standard C++</a> library.</td><td></td></tr></tbody></table></div></blockquote><h2 id=programming-interface>Programming Interface</h2><p>CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device.
It consists of a <strong>minimal set of extensions</strong> to the C++ language and a <strong>runtime</strong> library.
Any source file that contains some of these extensions must be compiled with <code>nvcc</code>.
The runtime provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. 
The runtime is built on top of <strong>a lower-level C API, the CUDA driver API</strong>, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as <strong>CUDA contexts</strong> - the analogue of host processes for the device - and <strong>CUDA modules</strong> - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this <strong>additional</strong> level of control and when using the runtime, context and module management are <strong>implicit</strong>, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where <strong>needed</strong>.</p><h3 id=nvcc>NVCC</h3><p>Kernels can be written using the CUDA instruction set architecture, called <em><strong>PTX</strong></em>, which is described in the PTX reference manual. It is however usually <strong>more effective</strong> to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc to execute on the device.
nvcc is a compiler driver that simplifies the process of compiling <em>C++</em> or <em>PTX</em> code: It provides simple and familiar command line options and executes them by invoking the collection of tools that implement the different compilation stages. 
Only a subset of C++ is fully supported for the device code.</p><blockquote><p>CUDA设备代码（device code）仅支持部分C++语法，这是由GPU硬件架构、执行模型和编译器设计的本质差异决定的。</p></blockquote><h4 id=offline-compilation>Offline Compilation</h4><pre><code>Source files compiled with nvcc can include a mix of host code (i.e., code that executes on the host) and device code (i.e., code that executes on the device). nvcc’s basic workflow consists in **separating** device code from host code and then:
</code></pre><ul><li>compiling the device code into an <strong>assembly form</strong> (<em>PTX</em> code) and/or <strong>binary form</strong> (<em>cubin</em> object),</li><li>and modifying the host code by <strong>replacing</strong> the &#171;&lt;&mldr;&#187;> syntax introduced in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#kernels target=_blank rel=noopener>Kernels</a> (and described in more details in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration target=_blank rel=noopener>Execution Configuration</a>) by the necessary CUDA runtime function calls to load and launch each compiled kernel <strong>from</strong> the <em>PTX</em> code and/or <em>cubin</em> object.
The modified host code is output either as C++ code that is left to be compiled using another tool or as object code directly by letting nvcc invoke the host compiler during the last compilation stage.
Applications can then:</li><li>Either link to the compiled host code (this is the most common case),</li><li>Or <strong>ignore</strong> the modified host code (if any) and use the CUDA driver API (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#driver-api target=_blank rel=noopener>Driver API</a>) to load and execute the <em>PTX</em> code or <em>cubin</em> object.</li></ul><h4 id=just-in-time-compilation>Just-in-Time Compilation</h4><p>Any <em>PTX</em> code loaded by an application at runtime is <strong>compiled further to binary code</strong> by the device driver. This is called <em>just-in-time compilation</em>. Just-in-time compilation increases application load time, but allows the application to <strong>benefit</strong> from any new compiler improvements coming with each new device driver. It is also the only way for applications to run on devices that <strong>did not exist</strong> at the time the application was compiled.
As an alternative to using <code>nvcc</code> to compile CUDA C++ device code, <strong>NVRTC</strong> can be used to compile CUDA C++ device code to PTX at runtime.</p><h4 id=compatibility>Compatibility</h4><p>Binary code is <strong>architecture-specific.</strong> A <em>cubin</em> object is generated using the compiler option <code>-code</code> that specifies the targeted architecture: For example, compiling with <code>-code=sm_80</code> produces binary code for devices of <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability target=_blank rel=noopener>compute capability</a> 8.0. Binary compatibility is guaranteed from one minor revision to the next one, but not from one minor revision to the previous one or across major revisions. In other words, a <em>cubin</em> object generated for compute capability <em>X.y</em> will only execute on devices of compute capability <em>X.z</em> where <em>z≥y</em>.
Some <em>PTX</em> instructions are <strong>only supported on devices of higher compute capabilities</strong>. For example, <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions target=_blank rel=noopener>Warp Shuffle Functions</a> are only supported on devices of compute capability 5.0 and above. The <code>-arch</code> compiler option <strong>specifies the compute capability that is assumed when compiling C++ to <em>PTX</em> code</strong>. So, code that contains warp shuffle, for example, must be compiled with -arch=compute_50 (or higher).
<em>PTX</em> code produced for some specific compute capability can always be compiled to binary code of <strong>greater or equal</strong> compute capability. Note that a binary compiled from an earlier PTX version <strong>may not</strong> make use of some hardware features. For example, a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal. As a result, the final binary may <strong>perform worse</strong> than would be possible if the binary were generated using the latest version of PTX.
<em>PTX</em> code compiled to target <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#architecture-specific-features target=_blank rel=noopener>Architecture-Specific Features</a> only runs on <strong>the exact same</strong> physical architecture and nowhere else. Architecture-specific <em>PTX</em> code is not forward and backward compatible. Example code compiled with sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible.
<em>PTX</em> code compiled to target <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#family-specific-features target=_blank rel=noopener>Family-Specific Features</a> only runs on the exact same physical architecture and other architectures in the same family. Family-specific <em>PTX</em> code is <strong>forward</strong> compatible with other devices in the same family, and is not backward compatible. Example code compiled with sm_100f or compute_100f only runs on devices with compute capability 10.0 and 10.3.
Which <em>PTX</em> and binary code gets embedded in a CUDA C++ application is controlled by the -arch and -code compiler options or the -gencode compiler option.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvcc x.cu -gencode <span class=nv>arch</span><span class=o>=</span>compute_50,code<span class=o>=</span>sm_50
</span></span></code></pre></div><p>Applications using the driver API <strong>must</strong> compile code to separate files and explicitly load and execute the most appropriate file at runtime.</p><h3 id=runtime>Runtime</h3><p>The runtime is implemented in the cudart library, which is linked to the application, either <strong>statically</strong> via <code>cudart.lib</code> or <code>libcudart.a</code>, or dynamically via <code>cudart.dll</code> or <code>libcudart.so</code>. Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime.</p><blockquote><p>cudart.lib 和 libcudart.a 都是 <strong>CUDA Runtime 库的静态链接版本</strong>，核心区别在于 <strong>操作系统平台和编译工具链的兼容性</strong>。
<strong>cudart.lib</strong> / <strong>cudart.dll</strong>: windows
<strong>libcudart.a</strong> / cudart.so : Linux / macos
All its entry points are <strong>prefixed with cuda</strong>.</p></blockquote><h4 id=initialization>Initialization</h4><p>CUDA 12.0</p><ul><li><strong>核心变化</strong><ul><li><strong>cudaInitDevice() 与 cudaSetDevice() <strong>在CUDA 12.0中成为</strong>显式初始化入口</strong>。调用二者之一会立即：✅ 初始化CUDA运行时库✅ 创建指定设备的Primary Context（主上下文）</li><li><strong>未调用时的默认行为</strong> ：运行时自动选择device 0，并在首次需要时隐式初始化（如调用cudaMalloc或内核启动）</li></ul></li><li><strong>工程意义</strong><ul><li><strong>性能分析</strong> ：显式初始化将耗时集中在可控阶段，避免首次API调用的延迟干扰计时。</li><li><strong>错误处理</strong> ：必须检查cudaSetDevice()的返回值（如cudaError_t），因其可能返回设备初始化错误（如cudaErrorInvalidDevice）
历史行为（CUDA 11.x及更早）</li></ul></li><li><strong>cudaSetDevice()的局限性 <strong>：仅设置当前设备，</strong> 不触发运行时初始化</strong>。运行时需通过其他API调用被动初始化。</li><li><strong>cudaFree(0)的妙用 <strong>：开发者调用此"空操作"函数（释放空指针）作为</strong>显式初始化触发器</strong>，目的包括：✅ 隔离初始化耗时（方便性能分析）✅ 提前捕获初始化错误（避免首次业务API调用失败）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>cudaSetDevice</span><span class=p>(</span><span class=mi>0</span><span class=p>);</span>        <span class=c1>// 设置设备（不初始化运行时）
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaFree</span><span class=p>(</span><span class=mi>0</span><span class=p>);</span>             <span class=c1>// 强制初始化运行时并检查错误
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaError_t</span> <span class=n>err</span> <span class=o>=</span> <span class=n>cudaGetLastError</span><span class=p>();</span>  <span class=c1>// 验证初始化状态
</span></span></span></code></pre></div><p>The runtime creates a CUDA context for each device in the system (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#context target=_blank rel=noopener>Context</a> for more details on CUDA contexts). This context is the <strong>primary context</strong> for this device and is initialized at <strong>the first runtime</strong> function which requires an active context on this device. It is <strong>shared among all the host threads</strong> of the application. As part of this context creation, the device code is just-in-time compiled if necessary (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#just-in-time-compilation target=_blank rel=noopener>Just-in-Time Compilation</a>) and loaded into device memory. This all happens transparently.</p><h4 id=device-memory>Device Memory</h4><p>Device memory can be allocated either as <em><strong>linear memory</strong></em> or as <em><strong>CUDA arrays</strong></em>.
CUDA arrays are opaque memory layouts optimized for texture fetching. They are described in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory target=_blank rel=noopener>Texture and Surface Memory</a>.</p><blockquote><ul><li><strong>不透明内存布局（Opaque Memory Layout）</strong> ：CUDA数组的物理存储结构对开发者完全隐藏，无法通过指针直接访问内部数据。其布局由GPU驱动动态优化，专为<strong>纹理拾取（Texture Fetching）</strong> 场景设计。</li><li><strong>纹理/表面内存优化</strong> ：数据以适合纹理缓存的格式存储，支持硬件加速的<strong>坐标寻址、滤波（如双线性插值）和边界处理</strong> （钳位/循环模式）。</li><li><strong>多维数据结构</strong> ：天然支持<strong>一维、二维或三维</strong>数据（如图像、体渲染数据），无需手动计算内存步长（Pitch）。
Linear memory is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list. 
支持任意数据结构，但<strong>多维数据需对齐</strong> （用cudaMallocPitch/cudaMalloc3D避免Bank Conflict）
Linear memory is typically allocated using <code>cudaMalloc()</code> and freed using <code>cudaFree()</code> and data transfer between host memory and device memory are typically done using <code>cudaMemcpy()</code>.
Linear memory can also be allocated through <code>cudaMallocPitch()</code> and <code>cudaMalloc3D()</code>. These functions are recommended for allocations of 2D or 3D arrays as it makes sure that the allocation is <strong>appropriately padded</strong> to meet the alignment requirements described in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses target=_blank rel=noopener>Device Memory Accesses</a>, therefore <strong>ensuring best performance</strong> when accessing the row addresses or performing copies between 2D arrays and other regions of device memory (using the <code>cudaMemcpy2D()</code> and <code>cudaMemcpy3D()</code> functions). The returned pitch (or stride) must be used to access array elements. </li></ul></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Host code
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>width</span> <span class=o>=</span> <span class=mi>64</span><span class=p>,</span> <span class=n>height</span> <span class=o>=</span> <span class=mi>64</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>float</span><span class=o>*</span> <span class=n>devPtr</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>size_t</span> <span class=n>pitch</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaMallocPitch</span><span class=p>(</span><span class=o>&amp;</span><span class=n>devPtr</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>pitch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>width</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>),</span> <span class=n>height</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>MyKernel</span><span class=o>&lt;&lt;&lt;</span><span class=mi>100</span><span class=p>,</span> <span class=mi>512</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>devPtr</span><span class=p>,</span> <span class=n>pitch</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>height</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Device code
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>MyKernel</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>devPtr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>size_t</span> <span class=n>pitch</span><span class=p>,</span> <span class=kt>int</span> <span class=n>width</span><span class=p>,</span> <span class=kt>int</span> <span class=n>height</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>r</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>r</span> <span class=o>&lt;</span> <span class=n>height</span><span class=p>;</span> <span class=o>++</span><span class=n>r</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span><span class=o>*</span> <span class=n>row</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)((</span><span class=kt>char</span><span class=o>*</span><span class=p>)</span><span class=n>devPtr</span> <span class=o>+</span> <span class=n>r</span> <span class=o>*</span> <span class=n>pitch</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>c</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>c</span> <span class=o>&lt;</span> <span class=n>width</span><span class=p>;</span> <span class=o>++</span><span class=n>c</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=kt>float</span> <span class=n>element</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=n>c</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><blockquote><p>Pitch: <strong>每行实际分配的字节数</strong></p></blockquote><h4 id=shared-memory>Shared Memory</h4><p><img src=https://i.ibb.co/jNV47KT/image-6.png loading=lazy>
Thread Block 中的每个 Thread 合力进行数据加载，一次加载一块</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Matrices are stored in row-major order:
</span></span></span><span class=line><span class=cl><span class=c1>// M(row, col) = *(M.elements + row * M.stride + col)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>typedef</span> <span class=k>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>width</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>height</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>stride</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span><span class=o>*</span> <span class=n>elements</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=n>Matrix</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// Get a matrix element
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__device__</span> <span class=kt>float</span> <span class=nf>GetElement</span><span class=p>(</span><span class=k>const</span> <span class=n>Matrix</span> <span class=n>A</span><span class=p>,</span> <span class=kt>int</span> <span class=n>row</span><span class=p>,</span> <span class=kt>int</span> <span class=n>col</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>A</span><span class=p>.</span><span class=n>elements</span><span class=p>[</span><span class=n>row</span> <span class=o>*</span> <span class=n>A</span><span class=p>.</span><span class=n>stride</span> <span class=o>+</span> <span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1>// Set a matrix element
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__device__</span> <span class=kt>void</span> <span class=nf>SetElement</span><span class=p>(</span><span class=n>Matrix</span> <span class=n>A</span><span class=p>,</span> <span class=kt>int</span> <span class=n>row</span><span class=p>,</span> <span class=kt>int</span> <span class=n>col</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                           <span class=kt>float</span> <span class=n>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>A</span><span class=p>.</span><span class=n>elements</span><span class=p>[</span><span class=n>row</span> <span class=o>*</span> <span class=n>A</span><span class=p>.</span><span class=n>stride</span> <span class=o>+</span> <span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1>// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is
</span></span></span><span class=line><span class=cl><span class=c1>// located col sub-matrices to the right and row sub-matrices down
</span></span></span><span class=line><span class=cl><span class=c1>// from the upper-left corner of A
</span></span></span><span class=line><span class=cl><span class=c1></span> <span class=n>__device__</span> <span class=n>Matrix</span> <span class=nf>GetSubMatrix</span><span class=p>(</span><span class=n>Matrix</span> <span class=n>A</span><span class=p>,</span> <span class=kt>int</span> <span class=n>row</span><span class=p>,</span> <span class=kt>int</span> <span class=n>col</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Matrix</span> <span class=n>Asub</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>Asub</span><span class=p>.</span><span class=n>width</span>    <span class=o>=</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>Asub</span><span class=p>.</span><span class=n>height</span>   <span class=o>=</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>Asub</span><span class=p>.</span><span class=n>stride</span>   <span class=o>=</span> <span class=n>A</span><span class=p>.</span><span class=n>stride</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>Asub</span><span class=p>.</span><span class=n>elements</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>A</span><span class=p>.</span><span class=n>elements</span><span class=p>[</span><span class=n>A</span><span class=p>.</span><span class=n>stride</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>*</span> <span class=n>row</span>
</span></span><span class=line><span class=cl>                                         <span class=o>+</span> <span class=n>BLOCK_SIZE</span> <span class=o>*</span> <span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>Asub</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1>// Thread block size
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#define BLOCK_SIZE 16
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>// Forward declaration of the matrix multiplication kernel
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>MatMulKernel</span><span class=p>(</span><span class=k>const</span> <span class=n>Matrix</span><span class=p>,</span> <span class=k>const</span> <span class=n>Matrix</span><span class=p>,</span> <span class=n>Matrix</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// Matrix multiplication - Host code
</span></span></span><span class=line><span class=cl><span class=c1>// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span> <span class=nf>MatMul</span><span class=p>(</span><span class=k>const</span> <span class=n>Matrix</span> <span class=n>A</span><span class=p>,</span> <span class=k>const</span> <span class=n>Matrix</span> <span class=n>B</span><span class=p>,</span> <span class=n>Matrix</span> <span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Load A and B to device memory
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>Matrix</span> <span class=n>d_A</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_A</span><span class=p>.</span><span class=n>width</span> <span class=o>=</span> <span class=n>d_A</span><span class=p>.</span><span class=n>stride</span> <span class=o>=</span> <span class=n>A</span><span class=p>.</span><span class=n>width</span><span class=p>;</span> <span class=n>d_A</span><span class=p>.</span><span class=n>height</span> <span class=o>=</span> <span class=n>A</span><span class=p>.</span><span class=n>height</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>size_t</span> <span class=n>size</span> <span class=o>=</span> <span class=n>A</span><span class=p>.</span><span class=n>width</span> <span class=o>*</span> <span class=n>A</span><span class=p>.</span><span class=n>height</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_A</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>d_A</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>A</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>Matrix</span> <span class=n>d_B</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_B</span><span class=p>.</span><span class=n>width</span> <span class=o>=</span> <span class=n>d_B</span><span class=p>.</span><span class=n>stride</span> <span class=o>=</span> <span class=n>B</span><span class=p>.</span><span class=n>width</span><span class=p>;</span> <span class=n>d_B</span><span class=p>.</span><span class=n>height</span> <span class=o>=</span> <span class=n>B</span><span class=p>.</span><span class=n>height</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>size</span> <span class=o>=</span> <span class=n>B</span><span class=p>.</span><span class=n>width</span> <span class=o>*</span> <span class=n>B</span><span class=p>.</span><span class=n>height</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_B</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>d_B</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>B</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Allocate C in device memory
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>Matrix</span> <span class=n>d_C</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_C</span><span class=p>.</span><span class=n>width</span> <span class=o>=</span> <span class=n>d_C</span><span class=p>.</span><span class=n>stride</span> <span class=o>=</span> <span class=n>C</span><span class=p>.</span><span class=n>width</span><span class=p>;</span> <span class=n>d_C</span><span class=p>.</span><span class=n>height</span> <span class=o>=</span> <span class=n>C</span><span class=p>.</span><span class=n>height</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>size</span> <span class=o>=</span> <span class=n>C</span><span class=p>.</span><span class=n>width</span> <span class=o>*</span> <span class=n>C</span><span class=p>.</span><span class=n>height</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_C</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Invoke kernel
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>dim3</span> <span class=n>dimBlock</span><span class=p>(</span><span class=n>BLOCK_SIZE</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>dim3</span> <span class=n>dimGrid</span><span class=p>(</span><span class=n>B</span><span class=p>.</span><span class=n>width</span> <span class=o>/</span> <span class=n>dimBlock</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>A</span><span class=p>.</span><span class=n>height</span> <span class=o>/</span> <span class=n>dimBlock</span><span class=p>.</span><span class=n>y</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>MatMulKernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>dimGrid</span><span class=p>,</span> <span class=n>dimBlock</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>d_A</span><span class=p>,</span> <span class=n>d_B</span><span class=p>,</span> <span class=n>d_C</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Read C from device memory
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>C</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>d_C</span><span class=p>.</span><span class=n>elements</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Free device memory
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>cudaFree</span><span class=p>(</span><span class=n>d_A</span><span class=p>.</span><span class=n>elements</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaFree</span><span class=p>(</span><span class=n>d_B</span><span class=p>.</span><span class=n>elements</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaFree</span><span class=p>(</span><span class=n>d_C</span><span class=p>.</span><span class=n>elements</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1>// Matrix multiplication kernel called by MatMul()
</span></span></span><span class=line><span class=cl><span class=c1></span> <span class=n>__global__</span> <span class=kt>void</span> <span class=nf>MatMulKernel</span><span class=p>(</span><span class=n>Matrix</span> <span class=n>A</span><span class=p>,</span> <span class=n>Matrix</span> <span class=n>B</span><span class=p>,</span> <span class=n>Matrix</span> <span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Block row and column
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>blockRow</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>blockCol</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Each thread block computes one sub-matrix Csub of C
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>Matrix</span> <span class=n>Csub</span> <span class=o>=</span> <span class=n>GetSubMatrix</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>blockRow</span><span class=p>,</span> <span class=n>blockCol</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Each thread computes one element of Csub
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// by accumulating results into Cvalue
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>float</span> <span class=n>Cvalue</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Thread row and column within Csub
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop over all the sub-matrices of A and B that are
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// required to compute Csub
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Multiply each pair of sub-matrices together
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// and accumulate the results
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>m</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>m</span> <span class=o>&lt;</span> <span class=p>(</span><span class=n>A</span><span class=p>.</span><span class=n>width</span> <span class=o>/</span> <span class=n>BLOCK_SIZE</span><span class=p>);</span> <span class=o>++</span><span class=n>m</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Get sub-matrix Asub of A
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>Matrix</span> <span class=n>Asub</span> <span class=o>=</span> <span class=n>GetSubMatrix</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>blockRow</span><span class=p>,</span> <span class=n>m</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Get sub-matrix Bsub of B
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>Matrix</span> <span class=n>Bsub</span> <span class=o>=</span> <span class=n>GetSubMatrix</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>m</span><span class=p>,</span> <span class=n>blockCol</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Shared memory used to store Asub and Bsub respectively
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>As</span><span class=p>[</span><span class=n>BLOCK_SIZE</span><span class=p>][</span><span class=n>BLOCK_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>Bs</span><span class=p>[</span><span class=n>BLOCK_SIZE</span><span class=p>][</span><span class=n>BLOCK_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Load Asub and Bsub from device memory to shared memory
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// Each thread loads one element of each sub-matrix
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>As</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>GetElement</span><span class=p>(</span><span class=n>Asub</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>Bs</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>GetElement</span><span class=p>(</span><span class=n>Bsub</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Synchronize to make sure the sub-matrices are loaded
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// before starting the computation
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Multiply Asub and Bsub together
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>e</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>e</span> <span class=o>&lt;</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span> <span class=o>++</span><span class=n>e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>Cvalue</span> <span class=o>+=</span> <span class=n>As</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>e</span><span class=p>]</span> <span class=o>*</span> <span class=n>Bs</span><span class=p>[</span><span class=n>e</span><span class=p>][</span><span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Synchronize to make sure that the preceding
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// computation is done before loading two new
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// sub-matrices of A and B in the next iteration
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Write Csub to device memory
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Each thread writes one element
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>SetElement</span><span class=p>(</span><span class=n>Csub</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>,</span> <span class=n>Cvalue</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h4 id=distributed-shared-memory>Distributed Shared Memory</h4><p>Accessing data in distributed shared memory requires all the thread blocks to exist. A user can <strong>guarantee</strong> that all thread blocks have started executing using <code>cluster.sync()</code> from <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cluster-group-cg target=_blank rel=noopener>Cluster Group</a> API. The user also needs to ensure that all distributed shared memory operations happen <strong>before the exit</strong> of a thread block, e.g., if a remote thread block is trying to read a given thread block’s shared memory, user needs to ensure that the shared memory read by remote thread block is completed before it can exit.</p><h4 id=page-locked-host-memory>Page-Locked Host Memory</h4><p>Page-locked host memory（页锁定主机内存，也称为 “固定内存” 或 “ pinned memory”）是一种特殊的主机内存分配方式，在 CUDA 编程中具有重要性能优化作用。</p><ul><li><strong>普通内存（pageable memory）</strong>：由操作系统动态管理，可被换出到虚拟内存，CPU 访问时需通过页表映射。</li><li><strong>页锁定内存</strong>：通过cudaHostAlloc或cudaHostRegister等 API 分配，其物理地址在内存中保持固定，<strong>不会被操作系统换出</strong>，且 CPU 和 GPU 可直接访问其物理地址。
页锁定内存绕过了操作系统的虚拟内存管理机制，直接映射到物理内存，避免了页面调度（page fault）的开销。</li></ul><h4 id=memory-synchronization>Memory Synchronization</h4><p>As the GPU cannot know at the time of execution which writes have been guaranteed at the source level to be visible and which are visible only by chance timing, it must cast a conservatively wide net for in-flight memory operations.
This sometimes leads to interference: because the GPU is waiting on memory operations it is not required to at the source level, the fence/flush may take longer than necessary.</p><h3 id=asynchronous-concurrent-execution>Asynchronous Concurrent Execution</h3><p>CUDA exposes the following operations as independent tasks that can operate concurrently with one another:</p><ul><li>Computation on the host;</li><li>Computation on the device;</li><li>Memory transfers from the host to the device;</li><li>Memory transfers from the device to the host;</li><li>Memory transfers within the memory of a given device;</li><li>Memory transfers among devices.
The level of concurrency achieved between these operations will depend on the feature set and compute capability of the device
Using asynchronous calls, many device operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available. This relieves the host thread of much of the responsibility to manage the device, leaving it free for other tasks.
Kernel launches are <strong>synchronous</strong> if hardware counters are collected via a <strong>profiler</strong> (Nsight, Visual Profiler) unless concurrent kernel profiling is enabled. Async memory copies might also be synchronous if they involve host memory that is <strong>not page-locked</strong>.
Concurrent host execution is facilitated through asynchronous library functions that <strong>return control to the host thread before the device completes the requested task</strong>. Using asynchronous calls, many device operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available. 
Programmers can globally disable asynchronicity of kernel launches for all CUDA applications running on a system by setting the <strong>CUDA_LAUNCH_BLOCKING</strong> environment variable to 1. This feature is provided for <strong>debugging purposes only</strong> and should not be used as a way to make production software run reliably.
A kernel from one CUDA context cannot execute concurrently with a kernel from another CUDA context. The GPU may time slice to provide forward progress to each context. If a user wants to run kernels from multiple process simultaneously on the SM, one must <strong>enable MPS</strong>.</li></ul><h3 id=stream>Stream</h3><p>Applications manage the concurrent operations described above through <em><strong>streams</strong></em>. A stream is <strong>a sequence of commands</strong> (possibly issued by <strong>different</strong> host threads) that execute in order. Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently; this behavior is not guaranteed and should therefore not be relied upon for correctness (for example, inter-kernel communication is undefined). The commands issued on a stream may execute <strong>when all the dependencies of the command are met</strong>. The dependencies could be previously launched commands on <strong>same</strong> stream or dependencies from <strong>other</strong> streams. The successful completion of <strong>synchronize</strong> call guarantees that all the commands launched are completed.</p><h4 id=default-stream>default stream</h4><p>Kernel launches and host device memory copies that <strong>do not specify</strong> any stream parameter, or equivalently that set the stream parameter to <strong>zero</strong>, are issued to the <strong>default</strong> stream. They are therefore executed in order.
For code that is compiled using the <code>--default-stream per-thread</code> compilation flag (or that defines the <code>CUDA_API_PER_THREAD_DEFAULT_STREAM</code> macro before including CUDA headers (cuda.h and cuda_runtime.h)), the default stream is a regular stream and each host thread has its own default stream.
For code that is compiled using the <code>—default-stream legacy</code> compilation flag, the default stream is a special stream called the <em><strong>NULL stream</strong></em> and each device has a single NULL stream used for all host threads. The NULL stream is special as it causes implicit synchronization.
For code that is compiled without specifying a <code>—default-stream compilation</code> flag, <code>—default-stream legacy</code> is assumed as the <strong>default</strong>.</p><h4 id=explicit-synchronization>explicit synchronization</h4><p>There are various ways to explicitly synchronize streams with each other.</p><ul><li><strong>cudaDeviceSynchronize</strong>() waits until all preceding commands in all streams of all host threads have completed.</li><li><strong>cudaStreamSynchronize</strong>()takes a stream as a parameter and waits until all preceding commands in the given stream have completed. It can be used to synchronize the host with a specific stream, allowing other streams to continue executing on the device.</li><li><strong>cudaStreamWaitEvent</strong>()takes a stream and an event as parameters (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#events target=_blank rel=noopener>Events</a> for a description of events)and makes all the commands added to the given stream after the call to cudaStreamWaitEvent()delay their execution until the given event has completed.</li><li><strong>cudaStreamQuery</strong>()provides applications with a way to know if all preceding commands in a stream have completed.</li></ul><h4 id=implicit-synchronization>implicit synchronization</h4><p>Two operations from different streams cannot run concurrently if any CUDA operation on the <strong>NULL stream is submitted in-between</strong> them, <strong>unless</strong> the streams are <strong>non-blocking streams</strong> (created with the cudaStreamNonBlocking flag).
Applications should follow these guidelines to improve their potential for concurrent kernel execution:</p><ul><li>All independent operations should be issued before dependent operations,</li><li>Synchronization of any kind should be delayed as long as possible.</li></ul><ol><li>默认流（NULL流）的阻塞性</li></ol><ul><li><strong>NULL流的特性</strong> ：默认流（未显式指定流时使用的流）具有<strong>隐式同步作用</strong>。当主机向NULL流提交操作（如cudaMemcpy或核函数）时，它会强制等待<strong>所有先前提交到任何流中的操作完成</strong>，自身执行结束后又会阻塞后续其他流的操作。</li><li><strong>并发中断原因</strong> ：若在两个不同流的操作之间插入NULL流操作（例如：StreamA操作 → NULL流操作 → StreamB操作），则：<ul><li>NULL流操作会等待StreamA操作完成才开始；</li><li>StreamB操作必须等待NULL流操作完成后才能启动。<em>结果</em>：StreamA和StreamB的操作被<strong>强制串行化</strong>，无法并发执行。</li></ul></li></ul><ol start=2><li>非阻塞流的例外</li></ol><ul><li><strong>创建方式</strong> ：使用cudaStreamCreateWithFlags(&amp;stream, cudaStreamNonBlocking)创建的流称为<strong>非阻塞流</strong> （Non-Blocking Stream）。</li><li><strong>规避阻塞</strong> ：非阻塞流<strong>不与NULL流同步</strong>，因此：<ul><li>即使NULL流操作插入在非阻塞流的操作之间（如NonBlockingStreamA操作 → NULL流操作 → NonBlockingStreamB操作），NonBlockingStreamA和NonBlockingStreamB的操作仍可<strong>并发执行</strong> 。</li><li>NULL流操作仅阻塞自身，不影响非阻塞流的独立性。</li></ul></li></ul><h4 id=host-functions><strong>Host Functions</strong></h4><p>The runtime provides a way to insert a CPU function call at any point into a stream via <strong>cudaLaunchHostFunc</strong>(). The provided function is executed on the host once all commands issued to the stream before the callback have completed.
A host function enqueued into a stream must not make CUDA API calls (directly or indirectly), as it might end up waiting on itself if it makes such a call leading to a deadlock.</p><h4 id=priority>Priority</h4><p>The relative priorities of streams can be specified at creation using cudaStreamCreateWithPriority(). The range of allowable priorities, ordered as [ greatest priority, least priority ] can be obtained using the cudaDeviceGetStreamPriorityRange() function.
These priorities serve as hints rather than guarantees.</p><h4 id=dependent-launch>Dependent Launch</h4><p>The <em>Programmatic Dependent Launch</em> mechanism allows for a dependent <em>secondary</em> kernel to launch <strong>before</strong> the <em>primary</em> kernel it depends on in the same CUDA stream has finished executing. Available starting with devices of <strong>compute capability 9.0</strong>, this technique can provide performance benefits when the <em>secondary</em> kernel can complete <strong>significant</strong> work that does not depend on the results of the <em>primary</em> kernel.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>primary_kernel</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=c1>// Initial work that should finish before starting secondary kernel
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>   <span class=c1>// Trigger the secondary kernel
</span></span></span><span class=line><span class=cl><span class=c1></span>   <span class=n>cudaTriggerProgrammaticLaunchCompletion</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   <span class=c1>// Work that can coincide with the secondary kernel
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>secondary_kernel</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=c1>// Independent work
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>   <span class=c1>// Will block until all primary kernels the secondary kernel is dependent on have completed and flushed results to global memory
</span></span></span><span class=line><span class=cl><span class=c1></span>   <span class=n>cudaGridDependencySynchronize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   <span class=c1>// Dependent work
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>cudaLaunchAttribute</span> <span class=n>attribute</span><span class=p>[</span><span class=mi>1</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>attribute</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=n>id</span> <span class=o>=</span> <span class=n>cudaLaunchAttributeProgrammaticStreamSerialization</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>attribute</span><span class=p>[</span><span class=mi>0</span><span class=p>].</span><span class=n>val</span><span class=p>.</span><span class=n>programmaticStreamSerializationAllowed</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>configSecondary</span><span class=p>.</span><span class=n>attrs</span> <span class=o>=</span> <span class=n>attribute</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>configSecondary</span><span class=p>.</span><span class=n>numAttrs</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>primary_kernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>grid_dim</span><span class=p>,</span> <span class=n>block_dim</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>stream</span><span class=o>&gt;&gt;&gt;</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=n>cudaLaunchKernelEx</span><span class=p>(</span><span class=o>&amp;</span><span class=n>configSecondary</span><span class=p>,</span> <span class=n>secondary_kernel</span><span class=p>);</span>
</span></span></code></pre></div><h3 id=events>Events</h3><p>The runtime also provides a way to closely monitor the device’s progress, as well as perform accurate timing, by letting the application asynchronously record <em><strong>events</strong></em> at any point in the program, and query when these events are completed. An event has completed when all tasks - or optionally, all commands in a given stream - preceding the event have completed. Events in <strong>stream zero</strong> are completed after all preceding tasks and commands in <strong>all streams</strong> are completed.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>cudaEvent_t</span> <span class=n>start</span><span class=p>,</span> <span class=n>stop</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventCreate</span><span class=p>(</span><span class=o>&amp;</span><span class=n>start</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventCreate</span><span class=p>(</span><span class=o>&amp;</span><span class=n>stop</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>cudaEventDestroy</span><span class=p>(</span><span class=n>start</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventDestroy</span><span class=p>(</span><span class=n>stop</span><span class=p>);</span>
</span></span></code></pre></div><p>Timing</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>cudaEventRecord</span><span class=p>(</span><span class=n>start</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpyAsync</span><span class=p>(</span><span class=n>inputDev</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>size</span><span class=p>,</span> <span class=n>inputHost</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>size</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>,</span> <span class=n>stream</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>MyKernel</span><span class=o>&lt;&lt;&lt;</span><span class=mi>100</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>stream</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>&gt;&gt;&gt;</span>
</span></span><span class=line><span class=cl>               <span class=p>(</span><span class=n>outputDev</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>size</span><span class=p>,</span> <span class=n>inputDev</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>size</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpyAsync</span><span class=p>(</span><span class=n>outputHost</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>size</span><span class=p>,</span> <span class=n>outputDev</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>size</span><span class=p>,</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>,</span> <span class=n>stream</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventRecord</span><span class=p>(</span><span class=n>stop</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventSynchronize</span><span class=p>(</span><span class=n>stop</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kt>float</span> <span class=n>elapsedTime</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventElapsedTime</span><span class=p>(</span><span class=o>&amp;</span><span class=n>elapsedTime</span><span class=p>,</span> <span class=n>start</span><span class=p>,</span> <span class=n>stop</span><span class=p>);</span>
</span></span></code></pre></div><h3 id=error-checking>Error Checking</h3><p><strong>All runtime functions return an error code</strong>, but for an asynchronous function (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution target=_blank rel=noopener>Asynchronous Concurrent Execution</a>), this error code cannot possibly report any of the asynchronous errors that could occur on the device since the function returns before the device has completed the task; the error code only reports errors that occur on the host prior to executing the task, typically related to parameter validation; if an asynchronous error occurs, it will be reported by some subsequent unrelated runtime function call.
The only way to check for asynchronous errors just after some asynchronous function call is therefore to <strong>synchronize just after the call</strong> by calling cudaDeviceSynchronize() (or by using any other synchronization mechanisms described in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution target=_blank rel=noopener>Asynchronous Concurrent Execution</a>) and checking the error code returned by cudaDeviceSynchronize().
The runtime maintains an error variable for each host thread that is initialized to cudaSuccess and is overwritten by the error code every time an error occurs (be it a parameter validation error or an asynchronous error). <strong>cudaPeekAtLastError</strong>() returns this variable. <strong>cudaGetLastError</strong>() returns this variable and <strong>resets</strong> it to cudaSuccess.
Kernel launches do not return any error code, so cudaPeekAtLastError() or cudaGetLastError() <strong>must be called just after the kernel launch to retrieve any pre-launch errors</strong>. To ensure that any error returned by cudaPeekAtLastError() or cudaGetLastError() does not originate from calls prior to the kernel launch, one has to make sure that the runtime error variable is set to cudaSuccess just before the kernel launch, for example, by calling cudaGetLastError() just before the kernel launch. Kernel launches are asynchronous, so to check for asynchronous errors, the application must <strong>synchronize in-between</strong> the kernel launch and the call to cudaPeekAtLastError() or cudaGetLastError().
Note that cudaErrorNotReady that may be returned by cudaStreamQuery() and cudaEventQuery() is not considered an error and is therefore not reported by cudaPeekAtLastError() or cudaGetLastError().</p><blockquote><p><strong>设计逻辑</strong> ：cudaErrorNotReady 是正常状态（表示“进行中”），而非错误</p></blockquote><h3 id=compute-mode>Compute Mode</h3><p>On Tesla solutions running Windows Server 2008 and later or Linux, one can set any device in a system in one of the three following modes using <strong>NVIDIA’s System Management Interface (nvidia-smi)</strong>, which is a tool distributed as part of the driver:</p><ul><li><strong><em>Default</em> compute mode</strong>: Multiple host threads can use the device (by calling cudaSetDevice() on this device, when using the runtime API, or by making current a context associated to the device, when using the driver API) at the same time.</li><li><strong><em>Exclusive-process</em> compute mode</strong>: Only one CUDA context may be created on the device across all processes in the system. The context may be current to as many threads as desired within the process that created that context.</li><li><strong><em>Prohibited</em> compute mode</strong>: No CUDA context can be created on the device.
⠀This means, in particular, that a host thread using the runtime API without explicitly calling cudaSetDevice() might be associated with a device <strong>other than device 0</strong> if device 0 turns out to be in prohibited mode or in exclusive-process mode and used by another process. cudaSetValidDevices() can be used to set a device from a prioritized list of devices.
Note also that, for devices featuring the Pascal architecture onwards (compute capability with major revision number 6 and higher), there exists support for <strong>Compute Preemption</strong>. This allows compute tasks to be preempted at instruction-level granularity, rather than thread block granularity as in prior Maxwell and Kepler GPU architecture, with the benefit that applications with long-running kernels can be prevented from either monopolizing the system or timing out. However, there will be <strong>context switch overheads</strong> associated with Compute Preemption, which is <strong>automatically enabled</strong> on those devices for which support exists. The individual attribute query function cudaDeviceGetAttribute() with the attribute <strong>cudaDevAttrComputePreemptionSupported</strong> can be used to determine if the device in use supports Compute Preemption. Users wishing to avoid context switch overheads associated with different processes can ensure that only one process is active on the GPU by <strong>selecting exclusive-process mode</strong>.
Applications may query the compute mode of a device by checking the computeMode device property (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration target=_blank rel=noopener>Device Enumeration</a>).</li></ul><h2 id=hardware-implementation>Hardware Implementation</h2><p>The NVIDIA GPU architecture is built around a <strong>scalable</strong> array of <strong>multithreaded <em>Streaming Multiprocessors</em></strong> (<em>SMs</em>). When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.
A multiprocessor is designed to execute hundreds of threads concurrently. To manage such a large number of threads, it employs a unique architecture called <em><strong>SIMT</strong></em> (<em>Single-Instruction, Multiple-Thread</em>) that is described in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture target=_blank rel=noopener>SIMT Architecture</a>. The instructions are pipelined, leveraging instruction-level parallelism within a single thread, as well as extensive thread-level parallelism through simultaneous hardware multithreading as detailed in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading target=_blank rel=noopener>Hardware Multithreading</a>. Unlike CPU cores, they are issued in order and there is no branch prediction or speculative execution.
The NVIDIA GPU architecture uses a <strong>little-endian</strong> representation.</p><h3 id=simt>SIMT</h3><p>The multiprocessor creates, manages, schedules, and executes threads in <strong>groups of 32 parallel threads</strong> called <em>warps</em>. Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently. The term <em>warp</em> originates from <strong>weaving</strong>, the first parallel thread technology. A <em>half-warp</em> is either the first or second half of a warp. A <em>quarter-warp</em> is either the first, second, third, or fourth quarter of a warp.
When a multiprocessor is <strong>given one or more thread blocks</strong> to execute, it partitions them into warps and each warp gets scheduled by a <em><strong>warp scheduler</strong></em> for execution. The way a block is partitioned into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0. <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy target=_blank rel=noopener>Thread Hierarchy</a> describes how thread IDs relate to thread indices in the block.
A warp executes one common instruction at a time, so <strong>full</strong> efficiency is realized when all 32 threads of a warp <strong>agree</strong> on their execution path. If threads of a warp diverge via a data-dependent conditional branch, the warp <strong>executes each</strong> branch path taken, disabling threads that are not on that path. Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjoint code paths.
The SIMT architecture is <strong>akin</strong> to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements. A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables programmers to write <strong>thread-level parallel</strong> code for independent, scalar threads, as well as data-parallel code for coordinated threads. For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance. Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually.
<strong>Prior to NVIDIA Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp</strong>. As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from.
Starting with the NVIDIA Volta architecture, <em><strong>Independent Thread Scheduling</strong></em> allows full concurrency between threads, regardless of warp. With Independent Thread Scheduling, the GPU maintains execution state per thread, including a program counter and call stack, and can yield execution at a per-thread granularity, either to make better use of execution resources or to allow one thread to wait for data to be produced by another. <strong>A schedule optimizer determines how to group active threads from the same warp together into SIMT units</strong>. This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity.
Independent Thread Scheduling can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity<a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fn2 target=_blank rel=noopener>2</a> of previous hardware architectures. In particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions) should be <strong>revisited</strong> to ensure compatibility with NVIDIA Volta and beyond. See <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x target=_blank rel=noopener>Compute Capability 7.x</a> for further details.</p><blockquote><p>The threads of a warp that are participating in the current instruction are called the <em><strong>active</strong></em> threads, whereas threads not on the current instruction are <em>inactive</em> (disabled). Threads can be inactive for a variety of reasons including having exited earlier than other threads of their warp, having taken a different branch path than the branch path currently executed by the warp, or being the last threads of a block whose number of threads is not a multiple of the warp size.
If a <strong>non-atomic</strong> instruction executed by a warp writes to the same location in global or shared memory for more than one of the threads of the warp, the number of serialized writes that occur to that location varies depending on the compute capability of the device (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x target=_blank rel=noopener>Compute Capability 5.x</a>, <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-6-x target=_blank rel=noopener>Compute Capability 6.x</a>, and <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x target=_blank rel=noopener>Compute Capability 7.x</a>), and which thread performs the final write is undefined.
If an <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions target=_blank rel=noopener>atomic</a> instruction executed by a warp reads, modifies, and writes to the same location in global memory for more than one of the threads of the warp, each read/modify/write to that location occurs and they are all serialized, but the order in which they occur is undefined.</p></blockquote><h3 id=hardware-multithreading>Hardware Multithreading</h3><p>The execution <strong>context</strong> (program counters, registers, and so on) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one execution context to another has <strong>no</strong> <strong>cost</strong>, and at every instruction issue time, a warp scheduler <strong>selects</strong> a warp that has threads ready to execute its next instruction (the <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture-notes target=_blank rel=noopener>active threads</a> of the warp) and issues the instruction to those threads.
In particular, each multiprocessor has a set of 32-bit registers that are partitioned among the warps, and a <em>parallel data cache</em> or <em><strong>shared memory</strong></em> that is partitioned among the thread blocks.
The number of blocks and warps that can reside and be processed together on the multiprocessor for a given kernel depends on the amount of registers and shared memory used by the kernel and the amount of registers and shared memory available on the multiprocessor. There are also a <strong>maximum number</strong> of resident blocks and a maximum number of resident warps per multiprocessor. These limits as well the amount of registers and shared memory available on the multiprocessor are a function of the compute capability of the device and are given in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities target=_blank rel=noopener>Compute Capabilities</a>. If there are not enough registers or shared memory available per multiprocessor to process <strong>at least one block</strong>, the kernel will fail to launch.</p><h2 id=cooperative-groups>Cooperative Groups</h2><p>Cooperative Groups is an extension to the CUDA programming model, introduced in <strong>CUDA 9</strong>, for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions.
Historically, the CUDA programming model has provided a single, simple construct for synchronizing cooperating threads: a barrier across all threads of a thread <strong>block</strong>, as implemented with the <strong>__syncthreads()</strong> intrinsic function. However, programmers would like to define and synchronize groups of threads at other granularities to enable greater performance, design flexibility, and software reuse in the form of “collective” group-wide function interfaces. In an effort to express broader patterns of parallel interaction, many performance-oriented programmers have resorted to writing their own ad hoc and unsafe primitives for synchronizing threads within a single warp, or across sets of thread blocks running on a single GPU. Whilst the performance improvements achieved have often been valuable, this has resulted in an ever-growing collection of brittle code that is expensive to write, tune, and maintain over time and across GPU generations. Cooperative Groups addresses this by providing a <strong>safe</strong> and <strong>future-proof</strong> mechanism to enable performant code.
header.h</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Primary header is compatible with pre-C++11, collective algorithm headers require C++11
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>// Optionally include for memcpy_async() collective
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups/memcpy_async.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>// Optionally include for reduce() collective
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups/reduce.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>// Optionally include for inclusive_scan() and exclusive_scan() collectives
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups/scan.h&gt;</span><span class=cp>
</span></span></span></code></pre></div><p>namespace</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>using</span> <span class=k>namespace</span> <span class=n>cooperative_groups</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// Alternatively use an alias to avoid polluting the namespace with collective algorithms
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>namespace</span> <span class=n>cg</span> <span class=o>=</span> <span class=n>cooperative_groups</span><span class=p>;</span>
</span></span></code></pre></div><h3 id=group-type>Group Type</h3><h4 id=implicit-groups>Implicit Groups</h4><p>Implicit groups represent the <strong>launch configuration</strong> of the kernel. Regardless of how your kernel is written, it always has a set number of threads, blocks and block dimensions, a single grid and grid dimensions. In addition, if the <strong>multi-device cooperative launch API</strong> is used, it can have multiple grids (single grid per device). These groups provide a starting point for decomposition into finer grained groups which are typically HW accelerated and are more specialized for the problem the developer is solving.
Although you can create an implicit group anywhere in the code, it is <strong>dangerous</strong> to do so. Creating a handle for an implicit group is a collective operation—<strong>all</strong> threads in the group must participate. If the group was created in a conditional branch that not all threads reach, this can lead to deadlocks or data corruption. For this reason, it is recommended that you create a handle for the implicit group <strong>upfront</strong> (as early as possible, before any branching has occurred) and use that handle throughout the kernel. Group handles must be initialized at declaration time (there is no default constructor) for the same reason and copy-constructing them is discouraged.</p><ul><li>Thread Block Group
Any CUDA programmer is already familiar with a certain group of threads: the thread block. The Cooperative Groups extension introduces a new datatype, <strong>thread_block</strong>, to explicitly represent this concept within the kernel.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>thread_block</span> <span class=n>g</span> <span class=o>=</span> <span class=n>this_thread_block</span><span class=p>();</span>
</span></span></code></pre></div><ul><li>Cluster Group
This group object represents all the threads launched in a single cluster. Refer to <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-block-clusters target=_blank rel=noopener>Thread Block Clusters</a>. The APIs are available on all hardware with Compute <strong>Capability 9.0+</strong>. In such cases, when a non-cluster grid is launched, the APIs assume a <strong>1x1x1</strong> cluster.</li><li>Grid Group
This group object represents all the threads launched in a single grid. APIs other than sync() are available at all times, but to be able to synchronize across the grid, you need to use the <strong>cooperative launch API</strong>.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>grid_group</span> <span class=n>g</span> <span class=o>=</span> <span class=n>this_grid</span><span class=p>();</span>
</span></span></code></pre></div><h4 id=explicit-groups>Explicit Groups</h4><ul><li>Thread Block Tile
A templated version of a tiled group, where a <strong>template parameter</strong> is used to specify the size of the tile - with this known at compile time there is the potential for more optimal execution.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>cooperative_kernel</span><span class=p>(...)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// obtain default &#34;current thread block&#34; group
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>thread_block</span> <span class=n>my_block</span> <span class=o>=</span> <span class=n>this_thread_block</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// subdivide into 32-thread, tiled subgroups
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Tiled subgroups evenly partition a parent group into
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// adjacent sets of threads - in this case each one warp in size
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>auto</span> <span class=n>my_tile</span> <span class=o>=</span> <span class=n>tiled_partition</span><span class=o>&lt;</span><span class=mi>32</span><span class=o>&gt;</span><span class=p>(</span><span class=n>my_block</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// This operation will be performed by only the
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// first 32-thread tile of each block
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>my_tile</span><span class=p>.</span><span class=n>meta_group_rank</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>my_tile</span><span class=p>.</span><span class=n>sync</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li>Single Thread Group</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>thread_block_tile</span><span class=o>&lt;</span><span class=mi>1</span><span class=o>&gt;</span> <span class=n>this_thread</span><span class=p>();</span>
</span></span></code></pre></div><p><strong>memcpy_async</strong> API uses a thread_group, to copy an int element from source to destination:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups/memcpy_async.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=n>cooperative_groups</span><span class=o>::</span><span class=n>memcpy_async</span><span class=p>(</span><span class=n>cooperative_groups</span><span class=o>::</span><span class=n>this_thread</span><span class=p>(),</span> <span class=n>dest</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>));</span>
</span></span></code></pre></div><ul><li>Coalesced Groups
In CUDA’s SIMT architecture, at the hardware level the multiprocessor executes threads in groups of 32 called warps. If there exists a data-dependent conditional branch in the application code such that threads within a warp diverge, then the warp serially executes each branch disabling threads not on that path. <strong>The threads that remain active on the path are referred to as coalesced</strong>. Cooperative Groups has functionality to discover, and create, a group containing all coalesced threads.
Constructing the group handle via <strong>coalesced_threads</strong>() is opportunistic. It returns the set of active threads at that point in time, and makes no guarantee about which threads are returned (as long as they are active) or that they will stay coalesced throughout execution (they will be brought back together for the execution of a collective but can diverge again afterwards).</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>coalesced_group</span> <span class=n>active</span> <span class=o>=</span> <span class=n>coalesced_threads</span><span class=p>();</span>
</span></span></code></pre></div><h3 id=group-partitioning>Group Partitioning</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>Size</span><span class=p>,</span> <span class=k>typename</span> <span class=n>ParentT</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>thread_block_tile</span><span class=o>&lt;</span><span class=n>Size</span><span class=p>,</span> <span class=n>ParentT</span><span class=o>&gt;</span> <span class=n>tiled_partition</span><span class=p>(</span><span class=k>const</span> <span class=n>ParentT</span><span class=o>&amp;</span> <span class=n>g</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>thread_group</span> <span class=nf>tiled_partition</span><span class=p>(</span><span class=k>const</span> <span class=n>thread_group</span><span class=o>&amp;</span> <span class=n>parent</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>tilesz</span><span class=p>);</span>
</span></span></code></pre></div><p>The <strong>tiled_partition</strong> method is a collective operation that partitions the parent group into a one-dimensional, row-major, tiling of subgroups. A total of ((size(parent)/tilesz) subgroups will be created, therefore the parent group size <strong>must be evenly divisible</strong> by the Size. The allowed parent groups are thread_block or thread_block_tile.
The implementation may cause the calling thread to <strong>wait</strong> until all the members of the parent group have invoked the operation before resuming execution. Functionality is limited to native hardware sizes, 1/2/4/8/16/32 and the cg::size(parent) must be greater than the Size parameter. The templated version of tiled_partition supports 64/128/256/512 sizes as well, but some additional steps are required on Compute Capability 7.5 or lower.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span> <span class=n>Label</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>coalesced_group</span> <span class=n>labeled_partition</span><span class=p>(</span><span class=k>const</span> <span class=n>coalesced_group</span><span class=o>&amp;</span> <span class=n>g</span><span class=p>,</span> <span class=n>Label</span> <span class=n>label</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>Size</span><span class=p>,</span> <span class=k>typename</span> <span class=n>Label</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>coalesced_group</span> <span class=n>labeled_partition</span><span class=p>(</span><span class=k>const</span> <span class=n>thread_block_tile</span><span class=o>&lt;</span><span class=n>Size</span><span class=o>&gt;&amp;</span> <span class=n>g</span><span class=p>,</span> <span class=n>Label</span> <span class=n>label</span><span class=p>);</span>
</span></span></code></pre></div><p><strong>Label</strong> can be any integral type.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>coalesced_group</span> <span class=nf>binary_partition</span><span class=p>(</span><span class=k>const</span> <span class=n>coalesced_group</span><span class=o>&amp;</span> <span class=n>g</span><span class=p>,</span> <span class=kt>bool</span> <span class=n>pred</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>Size</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>coalesced_group</span> <span class=n>binary_partition</span><span class=p>(</span><span class=k>const</span> <span class=n>thread_block_tile</span><span class=o>&lt;</span><span class=n>Size</span><span class=o>&gt;&amp;</span> <span class=n>g</span><span class=p>,</span> <span class=kt>bool</span> <span class=n>pred</span><span class=p>);</span>
</span></span></code></pre></div><p>This is a specialized form of <strong>labeled_partition</strong>(), where the label can only be 0 or 1.</p><h3 id=group-collectives>Group Collectives</h3><p>Cooperative Groups library provides a set of <strong>collective operations</strong> that can be performed by a group of threads. These operations require <strong>participation of all threads</strong> in the specified group in order to complete the operation. All threads in the group need to pass the <strong>same</strong> values for corresponding arguments to each collective call, unless different values are explicitly allowed in the argument description. Otherwise the behavior of the call is undefined.</p><h4 id=memcpy_async>memcpy_async</h4><p>memcpy_async is a <strong>group-wide</strong> collective memcpy that utilizes hardware accelerated support for non-blocking memory transactions <strong>from global to shared memory</strong>. Given a set of threads named in the group, memcpy_async will move specified amount of bytes or elements of the input type through a single pipeline stage. Additionally for achieving best performance when using the memcpy_async API, <strong>an alignment of 16 bytes</strong> for both shared memory and global memory is required. It is important to note that while this is a memcpy in the general case, it is only asynchronous if the source is global memory and the destination is shared memory and both <strong>can be addressed with 16, 8, or 4 byte alignments</strong>. Asynchronously copied data should only be read following a call to wait or wait_prior which signals that the corresponding stage has completed moving data to shared memory.
Having to wait on all outstanding requests can lose some flexibility (but gain simplicity). In order to efficiently overlap data transfer and execution, its important to be able to kick off an <strong>N+1</strong>memcpy_async request while waiting on and operating on request <strong>N</strong>. To do so, use memcpy_async and wait on it using the collective stage-based wait_prior API. </p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>/// This example streams elementsPerThreadBlock worth of data from global memory
</span></span></span><span class=line><span class=cl><span class=c1>/// into a limited sized shared memory (elementsInShared) block to operate on.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;cooperative_groups/memcpy_async.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=k>namespace</span> <span class=n>cg</span> <span class=o>=</span> <span class=n>cooperative_groups</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>kernel</span><span class=p>(</span><span class=kt>int</span><span class=o>*</span> <span class=n>global_data</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>cg</span><span class=o>::</span><span class=n>thread_block</span> <span class=n>tb</span> <span class=o>=</span> <span class=n>cg</span><span class=o>::</span><span class=n>this_thread_block</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=n>size_t</span> <span class=n>elementsPerThreadBlock</span> <span class=o>=</span> <span class=mi>16</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=n>size_t</span> <span class=n>elementsInShared</span> <span class=o>=</span> <span class=mi>128</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>__shared__</span> <span class=kt>int</span> <span class=n>local_smem</span><span class=p>[</span><span class=n>elementsInShared</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>size_t</span> <span class=n>copy_count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>size_t</span> <span class=n>index</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>index</span> <span class=o>&lt;</span> <span class=n>elementsPerThreadBlock</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>cg</span><span class=o>::</span><span class=n>memcpy_async</span><span class=p>(</span><span class=n>tb</span><span class=p>,</span> <span class=n>local_smem</span><span class=p>,</span> <span class=n>elementsInShared</span><span class=p>,</span> <span class=n>global_data</span> <span class=o>+</span> <span class=n>index</span><span class=p>,</span> <span class=n>elementsPerThreadBlock</span> <span class=o>-</span> <span class=n>index</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>copy_count</span> <span class=o>=</span> <span class=n>min</span><span class=p>(</span><span class=n>elementsInShared</span><span class=p>,</span> <span class=n>elementsPerThreadBlock</span> <span class=o>-</span> <span class=n>index</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>cg</span><span class=o>::</span><span class=n>wait</span><span class=p>(</span><span class=n>tb</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Work with local_smem
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>index</span> <span class=o>+=</span> <span class=n>copy_count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=multi-device>Multi-Device</h3><p>In order to enable synchronization across multiple devices with Cooperative Groups, use of the <strong>cudaLaunchCooperativeKernelMultiDevice</strong> CUDA API is required. This, a significant departure from existing CUDA APIs, will allow a single host thread to launch a kernel across multiple devices.
Deprecation Notice: <strong>cudaLaunchCooperativeKernelMultiDevice</strong> has been deprecated in CUDA 11.3 for all devices. Example of an alternative approach can be found in the multi device conjugate gradient sample.
Optimal performance in multi-device synchronization is achieved by enabling peer access via <strong>cuCtxEnablePeerAccess</strong> or <strong>cudaDeviceEnablePeerAccess</strong> for all participating devices.</p><h2 id=c-language-extensions>C++ Language Extensions</h2><h3 id=function-execution-space-specifier>Function Execution Space Specifier</h3><h4 id=__global__>__global__</h4><p>The __global__ execution space specifier declares a function as <strong>being a kernel</strong>. Such a function is:</p><ul><li>Executed on the device,</li><li>Callable from the host,</li><li>Callable from the device for devices of compute capability 5.0 or higher 
A __global__ function must have <strong>void</strong> return type, and cannot be a member of a class.
Any call to a __global__ function must specify its <strong>execution configuration</strong> as described in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration target=_blank rel=noopener>Execution Configuration</a>.
A call to a __global__ function is <strong>asynchronous</strong>, meaning it returns before the device has completed its execution.</li></ul><h4 id=__device__>__device__</h4><p>The __device__ execution space specifier declares a function that is:</p><ul><li>Executed on the device,</li><li>Callable from the <strong>device only</strong>.</li></ul><h4 id=__host__>__host__ </h4><p>⠀The __host__ execution space specifier declares a function that is:</p><ul><li>Executed on the host,</li><li>Callable from the <strong>host only.</strong>
It is equivalent to declare a function <strong>with only</strong> the __host__ execution space specifier or to declare it <strong>without</strong> any of the __host__, __device__, or __global__ execution space specifier; in either case the function is compiled for the host only.
The __device__ and __host__ execution space specifiers can be used together however, in which case the function is <strong>compiled for both</strong> the host and the device. The __CUDA_ARCH__ macro introduced in <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility target=_blank rel=noopener>Application Compatibility</a> can be used to differentiate code paths between host and device:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__host__</span> <span class=n>__device__</span> <span class=nf>func</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl><span class=cp>#if __CUDA_ARCH__ &gt;= 800
</span></span></span><span class=line><span class=cl><span class=cp></span>   <span class=c1>// Device code path for compute capability 8.x
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#elif __CUDA_ARCH__ &gt;= 700
</span></span></span><span class=line><span class=cl><span class=cp></span>   <span class=c1>// Device code path for compute capability 7.x
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#elif __CUDA_ARCH__ &gt;= 600
</span></span></span><span class=line><span class=cl><span class=cp></span>   <span class=c1>// Device code path for compute capability 6.x
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#elif __CUDA_ARCH__ &gt;= 500
</span></span></span><span class=line><span class=cl><span class=cp></span>   <span class=c1>// Device code path for compute capability 5.x
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#elif !defined(__CUDA_ARCH__)
</span></span></span><span class=line><span class=cl><span class=cp></span>   <span class=c1>// Host code path
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#endif
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=p>}</span>
</span></span></code></pre></div><h4 id=inline>inline</h4><p>The compiler inlines any __device__ function when <strong>deemed</strong> appropriate.</p><ul><li>The __noinline__ function qualifier can be used as a hint for the compiler <strong>not to inline</strong> the function if possible.</li><li>The __forceinline__ function qualifier can be used to <strong>force</strong> the compiler to inline the function.</li><li>The __inline_hint__ qualifier enables more <strong>aggressive</strong> inlining in the compiler. Unlike __forceinline__, it does not imply that the function is inline. It can be used to improve inlining across modules when using LTO.
The __noinline__ and __forceinline__ function qualifiers cannot be used together, and neither function qualifier can be applied to an inline function.
Neither the __noinline__ nor the __forceinline__ function qualifier can be used with the __inline_hint__ function qualifier.</li></ul><h3 id=variable-memory-space-specifier>Variable Memory Space Specifier</h3><h4 id=__device__-1>__device__</h4><p>The __device__ memory space specifier declares a variable that <strong>resides on the device</strong>.
At most one of __constant__, __managed__ and __shared__ may be used together with __device__ to further denote which memory space the variable belongs to. If none of them is present, the variable:</p><ul><li>Resides in <strong>global</strong> memory space,</li><li>Has the lifetime of the CUDA context in which it is created,</li><li>Has a distinct object per device,</li><li>Is accessible from all the threads within the grid and from the host through the runtime library (cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol()).</li></ul><blockquote><p>GPU 上的线程可直接读写该变量；主机（CPU）需通过 CUDA Runtime API 间接访问</p></blockquote><h4 id=__shared__>__shared__</h4><p>The __shared__ memory space specifier, optionally used together with __device__, declares a variable that:</p><ul><li>Resides in the shared memory space of a thread block,</li><li>Has the lifetime of the block,</li><li>Has a <strong>distinct object per block</strong>,</li><li>Is only accessible from all the threads within the block,</li><li>Does not have a <strong>constant address</strong>.
⠀When declaring a variable in shared memory as an external array such as</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>extern</span> <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>shared</span><span class=p>[];</span>
</span></span></code></pre></div><p>the size of the array is <strong>determined at launch time</strong> (see <a class=link href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration target=_blank rel=noopener>Execution Configuration</a>). All variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the array must be <strong>explicitly managed</strong> through offsets.</p><h3 id=dim3>dim3</h3><p>This type is an integer vector type based on <strong>uint3</strong> that is used to specify dimensions. When defining a variable of type dim3, any component left unspecified is <strong>initialized to 1</strong>.</p><h3 id=built-in-variables>Built-in Variables</h3><ul><li>gridDim(dim3)</li><li>BlockDim(dim3)</li><li>blockIdx(uint3)</li><li>threadIdx(uint3)</li><li>warpSize(int)</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/cuda/>CUDA</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Oct 22, 2025 16:27 CST</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/mac.db_store-file/><div class=article-image><img src=/covers/cover20.jpg loading=lazy data-key data-hash=/covers/cover20.jpg></div><div class=article-details><h2 class=article-title>【Mac】.DB_Store file</h2></div></a></article><article><a href=/p/cudaconcepts/><div class=article-details><h2 class=article-title>【CUDA】Concepts</h2></div></a></article><article><a href=/p/cudallm-queries/><div class=article-details><h2 class=article-title>【CUDA】LLM Queries</h2></div></a></article><article><a href=/p/cudanotes/><div class=article-details><h2 class=article-title>【CUDA】Notes</h2></div></a></article><article><a href=/p/cudafunction/><div class=article-details><h2 class=article-title>【CUDA】Function</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 飞鸿踏雪泥</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>