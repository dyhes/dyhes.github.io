<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Context Length 大模型的 Context Length（上下文长度）是指模型在单次推理过程中能够处理的所有信息的总容量上限，通常以 Token（文本的最小处理单位）来衡量。它就像模型的一次性“工作记忆区”，决定了模型能同时看到多少内容来生成回复。\n下面这个表格汇总了上下文长度的核心组成部分，方便你快速了解：\n组成部位 包含内容 说明 用户输入部分 当前的提问、指令 即你本次向模型提出的问题或要求 多轮对话的历史记录 (之前的问答交替) 为了让模型理解上下文，系统会自动将之前的对话内容拼接起来传给模型 上传的文档、提供的参考资料或附加说明 例如让模型分析你上传的PDF、TXT等文件内容 模型输出部分 模型根据上述所有输入内容，正在生成的回答或续写内容 模型产生的新的文本输出 为了更直观地理解这些部分如何共同构成一次完整的模型交互，你可以参考下面的流程图，它展示了上下文长度在单次推理中的动态使用过程：\nflowchart TD A[用户发起一次请求] --> B[构成本次输入的Token<br>（当前问题 + 历史对话 + 上传文档）] A --> C[模型生成输出的Token<br>（正在产生的回答）] B --> D[输入Token计数] C --> E[输出Token计数] D --> F[输入Token数 + 输出Token数] E --> F F --> G{总计 ≤ 上下文长度限制？} G -- 是 --> H[✅ 处理成功<br>模型正常生成回答] G -- 否 --> I[❌ 处理失败<br>输入被截断或输出被中断] H --> J[进入下一轮对话循环] I --> J 从图表中可以看出，用户输入和模型输出共同消耗着有限的上下文长度预算。\n"><title>【Shopee】LLM Knowledge</title><link rel=canonical href=https://dyhes.github.io/p/shopeellm-knowledge/><link rel=stylesheet href=/scss/style.min.f7091bff8043bd3e53b22be6c05dd86b506e8dec4d0d75d249d2dfb0fe074a46.css><meta property='og:title' content="【Shopee】LLM Knowledge"><meta property='og:description' content="Context Length 大模型的 Context Length（上下文长度）是指模型在单次推理过程中能够处理的所有信息的总容量上限，通常以 Token（文本的最小处理单位）来衡量。它就像模型的一次性“工作记忆区”，决定了模型能同时看到多少内容来生成回复。\n下面这个表格汇总了上下文长度的核心组成部分，方便你快速了解：\n组成部位 包含内容 说明 用户输入部分 当前的提问、指令 即你本次向模型提出的问题或要求 多轮对话的历史记录 (之前的问答交替) 为了让模型理解上下文，系统会自动将之前的对话内容拼接起来传给模型 上传的文档、提供的参考资料或附加说明 例如让模型分析你上传的PDF、TXT等文件内容 模型输出部分 模型根据上述所有输入内容，正在生成的回答或续写内容 模型产生的新的文本输出 为了更直观地理解这些部分如何共同构成一次完整的模型交互，你可以参考下面的流程图，它展示了上下文长度在单次推理中的动态使用过程：\nflowchart TD A[用户发起一次请求] --> B[构成本次输入的Token<br>（当前问题 + 历史对话 + 上传文档）] A --> C[模型生成输出的Token<br>（正在产生的回答）] B --> D[输入Token计数] C --> E[输出Token计数] D --> F[输入Token数 + 输出Token数] E --> F F --> G{总计 ≤ 上下文长度限制？} G -- 是 --> H[✅ 处理成功<br>模型正常生成回答] G -- 否 --> I[❌ 处理失败<br>输入被截断或输出被中断] H --> J[进入下一轮对话循环] I --> J 从图表中可以看出，用户输入和模型输出共同消耗着有限的上下文长度预算。\n"><meta property='og:url' content='https://dyhes.github.io/p/shopeellm-knowledge/'><meta property='og:site_name' content='飞鸿踏雪泥'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Shopee'><meta property='article:tag' content='LLM'><meta property='article:published_time' content='2025-09-17T00:00:00+00:00'><meta property='article:modified_time' content='2025-09-30T19:46:00+08:00'><meta name=twitter:title content="【Shopee】LLM Knowledge"><meta name=twitter:description content="Context Length 大模型的 Context Length（上下文长度）是指模型在单次推理过程中能够处理的所有信息的总容量上限，通常以 Token（文本的最小处理单位）来衡量。它就像模型的一次性“工作记忆区”，决定了模型能同时看到多少内容来生成回复。\n下面这个表格汇总了上下文长度的核心组成部分，方便你快速了解：\n组成部位 包含内容 说明 用户输入部分 当前的提问、指令 即你本次向模型提出的问题或要求 多轮对话的历史记录 (之前的问答交替) 为了让模型理解上下文，系统会自动将之前的对话内容拼接起来传给模型 上传的文档、提供的参考资料或附加说明 例如让模型分析你上传的PDF、TXT等文件内容 模型输出部分 模型根据上述所有输入内容，正在生成的回答或续写内容 模型产生的新的文本输出 为了更直观地理解这些部分如何共同构成一次完整的模型交互，你可以参考下面的流程图，它展示了上下文长度在单次推理中的动态使用过程：\nflowchart TD A[用户发起一次请求] --> B[构成本次输入的Token<br>（当前问题 + 历史对话 + 上传文档）] A --> C[模型生成输出的Token<br>（正在产生的回答）] B --> D[输入Token计数] C --> E[输出Token计数] D --> F[输入Token数 + 输出Token数] E --> F F --> G{总计 ≤ 上下文长度限制？} G -- 是 --> H[✅ 处理成功<br>模型正常生成回答] G -- 否 --> I[❌ 处理失败<br>输入被截断或输出被中断] H --> J[进入下一轮对话循环] I --> J 从图表中可以看出，用户输入和模型输出共同消耗着有限的上下文长度预算。\n"><link rel="shortcut icon" href=/github.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_b567f26f71c49c33.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>飞鸿踏雪泥</a></h1><h2 class=site-description>没有记录，就没有发生</h2></div></header><ol class=menu-social><li><a href=https://leetcode.cn/u/dyhes/ target=_blank title=LeetCode rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 13h7.5"/><path d="M9.424 7.268l4.999-4.999"/><path d="M16.633 16.644l-2.402 2.415a3.189 3.189.0 01-4.524.0l-3.77-3.787a3.223 3.223.0 010-4.544l3.77-3.787a3.189 3.189.0 014.524.0l2.302 2.313"/></svg></a></li><li><a href=https://github.com/dyhes target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:dyheslin@gmail.com target=_blank title=Gmail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-gmail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li><li><a href=mailto:1325574784@qq.com target=_blank title=Mail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M13 19H5a2 2 0 01-2-2V7a2 2 0 012-2h14a2 2 0 012 2v5.5"/><path d="M3 7l9 6 9-6"/><path d="M19 16l-2 3h4l-2 3"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/categories/><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#context-length>Context Length</a><ol><li><a href=#-关于token的简单说明>💡 关于Token的简单说明</a></li><li><a href=#-重要注意事项>⚠️ 重要注意事项</a></li><li><a href=#总结与应用建议>总结与应用建议</a></li></ol></li><li><a href=#常用上下文>常用上下文</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nutrition/ style=background-color:#93b5cf;color:>积雪粮</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/shopeellm-knowledge/>【Shopee】LLM Knowledge</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Sep 17, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>2 minute read</time></div></footer></div></header><section class=article-content><h2 id=context-length>Context Length</h2><p>大模型的 <strong>Context Length</strong>（上下文长度）是指模型在单次推理过程中能够处理的所有信息的总容量上限，通常以 <strong>Token</strong>（文本的最小处理单位）来衡量。它就像模型的一次性“工作记忆区”，决定了模型能同时看到多少内容来生成回复。</p><p>下面这个表格汇总了上下文长度的核心组成部分，方便你快速了解：</p><div class=table-wrapper><table><thead><tr><th>组成部位</th><th>包含内容</th><th>说明</th></tr></thead><tbody><tr><td><strong>用户输入部分</strong></td><td>当前的提问、指令</td><td>即你本次向模型提出的问题或要求</td></tr><tr><td></td><td>多轮对话的历史记录 (之前的问答交替)</td><td>为了让模型理解上下文，系统会自动将之前的对话内容拼接起来传给模型</td></tr><tr><td></td><td>上传的文档、提供的参考资料或附加说明</td><td>例如让模型分析你上传的PDF、TXT等文件内容</td></tr><tr><td><strong>模型输出部分</strong></td><td>模型根据上述所有输入内容，正在生成的回答或续写内容</td><td>模型产生的新的文本输出</td></tr></tbody></table></div><p>为了更直观地理解这些部分如何共同构成一次完整的模型交互，你可以参考下面的流程图，它展示了上下文长度在单次推理中的动态使用过程：</p><pre tabindex=0><code>flowchart TD
    A[用户发起一次请求] --&gt; B[构成本次输入的Token&lt;br&gt;（当前问题 + 历史对话 + 上传文档）]
    A --&gt; C[模型生成输出的Token&lt;br&gt;（正在产生的回答）]
    
    B --&gt; D[输入Token计数]
    C --&gt; E[输出Token计数]
    
    D --&gt; F[输入Token数 + 输出Token数]
    E --&gt; F
    
    F --&gt; G{总计 ≤ 上下文长度限制？}
    
    G -- 是 --&gt; H[✅ 处理成功&lt;br&gt;模型正常生成回答]
    G -- 否 --&gt; I[❌ 处理失败&lt;br&gt;输入被截断或输出被中断]
    
    H --&gt; J[进入下一轮对话循环]
    I --&gt; J
</code></pre><p>从图表中可以看出，<strong>用户输入</strong>和<strong>模型输出</strong>共同消耗着有限的上下文长度预算。</p><h3 id=-关于token的简单说明>💡 关于Token的简单说明</h3><p>Token是模型处理文本的基本单位，理解它有助于你更好地把握上下文长度的消耗。</p><ul><li>对于<strong>英文</strong>，一个单词通常约为1个Token（如 &ldquo;hello&rdquo;），但长词可能被拆分（如 &ldquo;chatGPT&rdquo; 可能为2个Token）。</li><li>对于<strong>中文</strong>，一个汉字大致相当于1个Token（如“你好”约为2个Tokens）。</li><li><strong>标点符号、数字和空格</strong>等也都计算在内。</li></ul><p>粗略估算时，可以认为 <strong>1个Token约等于1个汉字</strong>的长度。</p><h3 id=-重要注意事项>⚠️ 重要注意事项</h3><p>了解以下几点，能帮助你更有效地使用模型：</p><ol><li><strong>硬性限制与动态消耗</strong>：上下文长度是一个<strong>硬性上限</strong>。如图表所示，单次交互中“输入+输出”的Token总数绝不能超过此限制，否则最早的信息会被丢弃（截断），以确保处理正常进行。</li><li><strong>多轮对话的累积效应</strong>：在多轮对话中，为了维持对话连贯性，<strong>之前所有轮次的对话历史（你的问题和模型的回答）都会作为新的输入信息的一部分，被再次传递给模型</strong>。这意味着对话轮数越多，消耗的上下文长度就越多，可用的剩余空间就越少。</li><li><strong>最大输出长度的约束</strong>：除了总长度限制，模型通常还有一个<strong>最大输出长度</strong>（Max Output Tokens）的限制，即单次回复能生成的最大Token数。例如，即使上下文长度有128K，但模型单次回答可能被限制为只能输出4K Token。因此，实际可用输入长度 ≈ 上下文总长度 - 最大输出长度。</li><li><strong>“失忆”现象的根源</strong>：当长对话累积的Token数超过模型的上下文长度限制时，系统会<strong>丢弃最早的信息</strong>（通常是对话开头部分）以容纳新输入。这就是为什么模型在长对话后可能“忘记”很久之前讨论过的内容，这并非模型本身故障，而是工程上的处理策略。</li></ol><h3 id=总结与应用建议>总结与应用建议</h3><p>理解上下文长度的组成和限制，能帮助你更高效地与大型语言模型交互：</p><ul><li><strong>处理长文档</strong>：若需分析长文档，可先提取关键章节或分段提交，以避免耗尽上下文空间。</li><li><strong>管理长对话</strong>：在长时间、多轮对话中，对于重要的前期结论或信息，可有意识地<strong>在后续提问中重申或提及</strong>，以防其因截断而被“遗忘”。</li><li><strong>控制输出长度</strong>：若需要模型生成非常长的内容（如长篇文章、报告），而它的单次输出长度有限，可以<strong>引导它分步骤、分章节地生成</strong>。</li></ul><p>希望这些信息能帮助你更好地理解和使用大模型。</p><h2 id=常用上下文>常用上下文</h2><p>了解当前主流大模型的上下文长度（Context Length）对于选择合适的模型至关重要。上下文长度决定了模型单次处理信息的容量，直接影响到处理长文档、维持长对话和复杂推理的能力。以下是截至2025年的流行大模型及其上下文长度信息，供你参考。</p><div class=table-wrapper><table><thead><tr><th>模型系列</th><th>模型名称 (或版本)</th><th>上下文长度 (Tokens)</th><th>备注/特点</th></tr></thead><tbody><tr><td><strong>国外模型</strong></td><td></td><td></td><td></td></tr><tr><td>OpenAI</td><td>GPT-4 (8K版)</td><td>8,192</td><td>支持约8k tokens输入输出</td></tr><tr><td></td><td>GPT-4 (32K版)</td><td>32,768</td><td>单次可处理约2.5万字中文或英文</td></tr><tr><td></td><td>GPT-5</td><td>1,000,000+</td><td>超长上下文处理能力达1M+ tokens</td></tr><tr><td>Anthropic</td><td>Claude 4</td><td>1,000,000 (1000k)</td><td>超低幻觉，适用于法律、医疗等高风险领域</td></tr><tr><td>Google DeepMind</td><td>Gemini 系列标准版</td><td>~32,768</td><td>性能接近GPT-4，未明确公布具体数值</td></tr><tr><td></td><td>Gemini 1.5 Flash</td><td>1,000,000</td><td>支持百万token上下文</td></tr><tr><td></td><td>Gemini 1.5 Pro</td><td>2,000,000</td><td>支持两百万token上下文</td></tr><tr><td></td><td>Gemini 2.5 Pro</td><td>1,000,000</td><td>液态神经网络架构，响应延迟低</td></tr><tr><td>Meta (Facebook)</td><td>Llama 2</td><td>4,096</td><td>相比Llama 1的2048 tokens翻倍</td></tr><tr><td></td><td>Llama 4</td><td>-</td><td>提供万亿参数版本，支持100+语言，手机端部署能力强</td></tr><tr><td>Mistral AI</td><td>Mistral-Next</td><td>-</td><td>混合专家（MoE）架构，效率高</td></tr><tr><td><strong>国内模型</strong></td><td></td><td></td><td></td></tr><tr><td>百度 (Baidu)</td><td>文心大模型 5.0 (ERNIE 5.0)</td><td>-</td><td>产业级多模态能力，融合行业数据库</td></tr><tr><td>阿里巴巴 (Alibaba)</td><td>通义千问3.0 (Qwen3)</td><td>500,000</td><td>超长文本支持500k tokens，电商优化能力突出</td></tr><tr><td></td><td>通义千问-Max</td><td>32,000~1,000,000</td><td>适合复杂任务，推理能力最强</td></tr><tr><td></td><td>通义千问-Plus</td><td>1,000,000</td><td>性能均衡</td></tr><tr><td></td><td>通义千问-Flash</td><td>1,000,000</td><td>适合简单任务，速度快、成本低</td></tr><tr><td>腾讯 (Tencent)</td><td>混元大模型3.0 (Hunyuan 3.0)</td><td>-</td><td>游戏NPC智能化，微信深度整合</td></tr><tr><td>华为 (Huawei)</td><td>盘古大模型4.0</td><td>-</td><td>行业专用小模型，端侧AI</td></tr><tr><td>科大讯飞 (iFLYTEK)</td><td>讯飞星火V4.0 Turbo</td><td>-</td><td>中英双语对齐优化，支持202种方言识别</td></tr><tr><td>深度求索 (DeepSeek)</td><td>DeepSeek-V3</td><td>128,000</td><td>混合推理架构，数学与代码能力突出</td></tr><tr><td>月之暗面 (Moonshot)</td><td>Kimi+</td><td>-</td><td>强调1000万字上下文（全球最长文本处理能力之一）</td></tr><tr><td>智谱AI (Zhipu AI)</td><td>GLM-5</td><td>-</td><td>中英双语对齐，开源版本增强</td></tr></tbody></table></div><p><strong>💡 重要说明：</strong></p><ol><li><strong>Token换算参考</strong>：通常，1个Token约等于1-2个汉字（中文）或0.75个单词（英文）。例如，32K tokens大约对应2.4万至4.8万个汉字。</li><li><strong>实际限制</strong>：模型声称的上下文长度是<strong>理论最大值</strong>。实际使用时，<strong>输入（你的问题+上下文）和输出（模型的回答）的Token数之和</strong>不能超过此限制。</li><li><strong>技术发展</strong>：上下文长度是当前大模型技术竞争的一个焦点，数值更新很快。部分模型（如GPT-5、Claude 4、Gemini 1.5/2.5 Pro）已支持<strong>百万级（1M+）甚至两百万（2M）Token</strong>，使其能够处理整本书、长篇论文或极其复杂的多轮对话。</li><li><strong>选择建议</strong>：<ul><li>处理日常问答、短文总结：<strong>8K-32K</strong> 的模型通常足够。</li><li>分析长篇报告、技术文档、代码库：建议选择 <strong>100K以上</strong> 的模型。</li><li>进行超长文献解析、书籍内容分析或极其复杂的多轮对话：应考虑 <strong>500K至1M+</strong> 的模型。</li></ul></li></ol><p>希望这份列表能帮助你更好地了解当前大模型的上下文长度能力。如果你有特定的应用场景，可以据此选择最适合的模型。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/shopee/>Shopee</a>
<a href=/tags/llm/>LLM</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Sep 30, 2025 19:46 CST</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/mac.db_store-file/><div class=article-image><img src=/covers/cover20.jpg loading=lazy data-key data-hash=/covers/cover20.jpg></div><div class=article-details><h2 class=article-title>【Mac】.DB_Store file</h2></div></a></article><article><a href=/p/shopeejavaparser/><div class=article-details><h2 class=article-title>【Shopee】JavaParser</h2></div></a></article><article><a href=/p/shopeegraph/><div class=article-details><h2 class=article-title>【Shopee】Graph</h2></div></a></article><article><a href=/p/shopeespring-fundamentals/><div class=article-details><h2 class=article-title>【Shopee】Spring Fundamentals</h2></div></a></article><article><a href=/p/shopeeconfig-priority/><div class=article-details><h2 class=article-title>【Shopee】Config Priority</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 飞鸿踏雪泥</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>