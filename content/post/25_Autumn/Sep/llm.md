---
title: 【Shopee】LLM Knowledge
date: 2025-09-17 00:00:00+0000
categories: [nutrition]
tags: [Shopee, LLM]
---

## Context Length

大模型的 **Context Length**（上下文长度）是指模型在单次推理过程中能够处理的所有信息的总容量上限，通常以 **Token**（文本的最小处理单位）来衡量。它就像模型的一次性“工作记忆区”，决定了模型能同时看到多少内容来生成回复。

下面这个表格汇总了上下文长度的核心组成部分，方便你快速了解：

| 组成部位         | 包含内容                                           | 说明                                                         |
| ---------------- | -------------------------------------------------- | ------------------------------------------------------------ |
| **用户输入部分** | 当前的提问、指令                                   | 即你本次向模型提出的问题或要求                               |
|                  | 多轮对话的历史记录 (之前的问答交替)                | 为了让模型理解上下文，系统会自动将之前的对话内容拼接起来传给模型 |
|                  | 上传的文档、提供的参考资料或附加说明               | 例如让模型分析你上传的PDF、TXT等文件内容                     |
| **模型输出部分** | 模型根据上述所有输入内容，正在生成的回答或续写内容 | 模型产生的新的文本输出                                       |

为了更直观地理解这些部分如何共同构成一次完整的模型交互，你可以参考下面的流程图，它展示了上下文长度在单次推理中的动态使用过程：

```
flowchart TD
    A[用户发起一次请求] --> B[构成本次输入的Token<br>（当前问题 + 历史对话 + 上传文档）]
    A --> C[模型生成输出的Token<br>（正在产生的回答）]
    
    B --> D[输入Token计数]
    C --> E[输出Token计数]
    
    D --> F[输入Token数 + 输出Token数]
    E --> F
    
    F --> G{总计 ≤ 上下文长度限制？}
    
    G -- 是 --> H[✅ 处理成功<br>模型正常生成回答]
    G -- 否 --> I[❌ 处理失败<br>输入被截断或输出被中断]
    
    H --> J[进入下一轮对话循环]
    I --> J
```

从图表中可以看出，**用户输入**和**模型输出**共同消耗着有限的上下文长度预算。

### 💡 关于Token的简单说明

Token是模型处理文本的基本单位，理解它有助于你更好地把握上下文长度的消耗。

- 对于**英文**，一个单词通常约为1个Token（如 "hello"），但长词可能被拆分（如 "chatGPT" 可能为2个Token）。
- 对于**中文**，一个汉字大致相当于1个Token（如“你好”约为2个Tokens）。
- **标点符号、数字和空格**等也都计算在内。

粗略估算时，可以认为 **1个Token约等于1个汉字**的长度。

### ⚠️ 重要注意事项

了解以下几点，能帮助你更有效地使用模型：

1. **硬性限制与动态消耗**：上下文长度是一个**硬性上限**。如图表所示，单次交互中“输入+输出”的Token总数绝不能超过此限制，否则最早的信息会被丢弃（截断），以确保处理正常进行。
2. **多轮对话的累积效应**：在多轮对话中，为了维持对话连贯性，**之前所有轮次的对话历史（你的问题和模型的回答）都会作为新的输入信息的一部分，被再次传递给模型**。这意味着对话轮数越多，消耗的上下文长度就越多，可用的剩余空间就越少。
3. **最大输出长度的约束**：除了总长度限制，模型通常还有一个**最大输出长度**（Max Output Tokens）的限制，即单次回复能生成的最大Token数。例如，即使上下文长度有128K，但模型单次回答可能被限制为只能输出4K Token。因此，实际可用输入长度 ≈ 上下文总长度 - 最大输出长度。
4. **“失忆”现象的根源**：当长对话累积的Token数超过模型的上下文长度限制时，系统会**丢弃最早的信息**（通常是对话开头部分）以容纳新输入。这就是为什么模型在长对话后可能“忘记”很久之前讨论过的内容，这并非模型本身故障，而是工程上的处理策略。

### 总结与应用建议

理解上下文长度的组成和限制，能帮助你更高效地与大型语言模型交互：

- **处理长文档**：若需分析长文档，可先提取关键章节或分段提交，以避免耗尽上下文空间。
- **管理长对话**：在长时间、多轮对话中，对于重要的前期结论或信息，可有意识地**在后续提问中重申或提及**，以防其因截断而被“遗忘”。
- **控制输出长度**：若需要模型生成非常长的内容（如长篇文章、报告），而它的单次输出长度有限，可以**引导它分步骤、分章节地生成**。

希望这些信息能帮助你更好地理解和使用大模型。

## 常用上下文

了解当前主流大模型的上下文长度（Context Length）对于选择合适的模型至关重要。上下文长度决定了模型单次处理信息的容量，直接影响到处理长文档、维持长对话和复杂推理的能力。以下是截至2025年的流行大模型及其上下文长度信息，供你参考。

| 模型系列            | 模型名称 (或版本)           | 上下文长度 (Tokens) | 备注/特点                                        |
| ------------------- | --------------------------- | ------------------- | ------------------------------------------------ |
| **国外模型**        |                             |                     |                                                  |
| OpenAI              | GPT-4 (8K版)                | 8,192               | 支持约8k tokens输入输出                          |
|                     | GPT-4 (32K版)               | 32,768              | 单次可处理约2.5万字中文或英文                    |
|                     | GPT-5                       | 1,000,000+          | 超长上下文处理能力达1M+ tokens                   |
| Anthropic           | Claude 4                    | 1,000,000 (1000k)   | 超低幻觉，适用于法律、医疗等高风险领域           |
| Google DeepMind     | Gemini 系列标准版           | ~32,768             | 性能接近GPT-4，未明确公布具体数值                |
|                     | Gemini 1.5 Flash            | 1,000,000           | 支持百万token上下文                              |
|                     | Gemini 1.5 Pro              | 2,000,000           | 支持两百万token上下文                            |
|                     | Gemini 2.5 Pro              | 1,000,000           | 液态神经网络架构，响应延迟低                     |
| Meta (Facebook)     | Llama 2                     | 4,096               | 相比Llama 1的2048 tokens翻倍                     |
|                     | Llama 4                     | -                   | 提供万亿参数版本，支持100+语言，手机端部署能力强 |
| Mistral AI          | Mistral-Next                | -                   | 混合专家（MoE）架构，效率高                      |
| **国内模型**        |                             |                     |                                                  |
| 百度 (Baidu)        | 文心大模型 5.0 (ERNIE 5.0)  | -                   | 产业级多模态能力，融合行业数据库                 |
| 阿里巴巴 (Alibaba)  | 通义千问3.0 (Qwen3)         | 500,000             | 超长文本支持500k tokens，电商优化能力突出        |
|                     | 通义千问-Max                | 32,000~1,000,000    | 适合复杂任务，推理能力最强                       |
|                     | 通义千问-Plus               | 1,000,000           | 性能均衡                                         |
|                     | 通义千问-Flash              | 1,000,000           | 适合简单任务，速度快、成本低                     |
| 腾讯 (Tencent)      | 混元大模型3.0 (Hunyuan 3.0) | -                   | 游戏NPC智能化，微信深度整合                      |
| 华为 (Huawei)       | 盘古大模型4.0               | -                   | 行业专用小模型，端侧AI                           |
| 科大讯飞 (iFLYTEK)  | 讯飞星火V4.0 Turbo          | -                   | 中英双语对齐优化，支持202种方言识别              |
| 深度求索 (DeepSeek) | DeepSeek-V3                 | 128,000             | 混合推理架构，数学与代码能力突出                 |
| 月之暗面 (Moonshot) | Kimi+                       | -                   | 强调1000万字上下文（全球最长文本处理能力之一）   |
| 智谱AI (Zhipu AI)   | GLM-5                       | -                   | 中英双语对齐，开源版本增强                       |

**💡 重要说明：**

1. **Token换算参考**：通常，1个Token约等于1-2个汉字（中文）或0.75个单词（英文）。例如，32K tokens大约对应2.4万至4.8万个汉字。
2. **实际限制**：模型声称的上下文长度是**理论最大值**。实际使用时，**输入（你的问题+上下文）和输出（模型的回答）的Token数之和**不能超过此限制。
3. **技术发展**：上下文长度是当前大模型技术竞争的一个焦点，数值更新很快。部分模型（如GPT-5、Claude 4、Gemini 1.5/2.5 Pro）已支持**百万级（1M+）甚至两百万（2M）Token**，使其能够处理整本书、长篇论文或极其复杂的多轮对话。
4. **选择建议**：
   - 处理日常问答、短文总结：**8K-32K** 的模型通常足够。
   - 分析长篇报告、技术文档、代码库：建议选择 **100K以上** 的模型。
   - 进行超长文献解析、书籍内容分析或极其复杂的多轮对话：应考虑 **500K至1M+** 的模型。

希望这份列表能帮助你更好地了解当前大模型的上下文长度能力。如果你有特定的应用场景，可以据此选择最适合的模型。